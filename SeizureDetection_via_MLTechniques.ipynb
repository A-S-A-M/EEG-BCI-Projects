{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1BHOj80Uzui_dgBd0d4DmUQVh9Xz9kaa6",
      "authorship_tag": "ABX9TyOMB+fPuDP0bH0nbSyVu5ae",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-S-A-M/EEG-BCI-Projects/blob/main/SeizureDetection_via_MLTechniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATLhk2rmaPWQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ------------------------\n",
        "# Configuration\n",
        "# ------------------------\n",
        "DATA_ROOT = r\"D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\"  # Root folder containing EEG data organized by class\n",
        "CHANNELS = ['F','N','O','S','Z']  # EEG classes / labels\n",
        "FIXED_LENGTH = 4096  # Number of timesteps per EEG sample for CNN input\n",
        "\n",
        "# Lists to store data and labels\n",
        "X = []  # EEG signals\n",
        "y = []  # Corresponding labels\n",
        "\n",
        "# ------------------------\n",
        "# Load and preprocess EEG signals\n",
        "# ------------------------\n",
        "for ch in CHANNELS:\n",
        "    folder = os.path.join(DATA_ROOT, ch)  # Path to current class folder\n",
        "    # Get all .txt or _filtered.txt files in folder\n",
        "    files = [f for f in os.listdir(folder) if f.endswith(\".txt\") or f.endswith(\"_filtered.txt\")]\n",
        "\n",
        "    for f in files:\n",
        "        path = os.path.join(folder, f)\n",
        "        signal = np.loadtxt(path)  # Load EEG signal as 1D numpy array\n",
        "\n",
        "        # Pad or truncate signals to FIXED_LENGTH\n",
        "        if len(signal) < FIXED_LENGTH:\n",
        "            # Pad with zeros at the end if signal is shorter\n",
        "            signal = np.pad(signal, (0, FIXED_LENGTH - len(signal)), 'constant')\n",
        "        else:\n",
        "            # Truncate if signal is longer than FIXED_LENGTH\n",
        "            signal = signal[:FIXED_LENGTH]\n",
        "\n",
        "        # Append processed signal and corresponding label\n",
        "        X.append(signal)\n",
        "        y.append(ch)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X = np.array(X)  # Shape: [samples, timesteps]\n",
        "y = np.array(y)  # Shape: [samples]\n",
        "\n",
        "# ------------------------\n",
        "# Encode labels as integers\n",
        "# ------------------------\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)  # Converts labels F,N,O,S,Z → 0,1,2,3,4\n",
        "\n",
        "# ------------------------\n",
        "# Add channel dimension for CNN input\n",
        "# ------------------------\n",
        "# CNN expects input shape: [samples, timesteps, channels]\n",
        "X_cnn = X[..., np.newaxis]  # Shape becomes [samples, timesteps, 1]\n",
        "\n",
        "# ------------------------\n",
        "# Split dataset into training and testing sets\n",
        "# ------------------------\n",
        "# 80% training, 20% testing, stratified to preserve label distribution\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_cnn, y_enc, test_size=0.2, random_state=42, stratify=y_enc\n",
        ")\n",
        "\n",
        "# ------------------------\n",
        "# Print dataset shapes\n",
        "# ------------------------\n",
        "print(\"Data shapes:\")\n",
        "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
        "print(\"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DATA REPO"
      ],
      "metadata": {
        "id": "juws75sTfmP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_root = '/content/drive/MyDrive/EEG_DATA_REPO'\n",
        "\n",
        "for root, dirs, files in os.walk(data_root):\n",
        "    print(f\"Directory: {root}\")\n",
        "    for file in files:\n",
        "        print(f\"  {file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NiqRoga4fsGs",
        "outputId": "6265077a-a15a-47b7-a87b-20f3171f6cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO\n",
            "  seizure_state_comparison.png\n",
            "  SeizureDetectionCode.ipynb\n",
            "  SeizDetect.ipynb\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEGDATA_EXTRACTED\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEGDATA_EXTRACTED/N\n",
            "  n012.txt\n",
            "  n055.txt\n",
            "  n087.txt\n",
            "  n027.txt\n",
            "  n079.txt\n",
            "  n097.txt\n",
            "  n047.txt\n",
            "  n081.txt\n",
            "  n092.txt\n",
            "  n090.txt\n",
            "  n085.txt\n",
            "  n076.txt\n",
            "  n006.txt\n",
            "  n036.txt\n",
            "  n017.txt\n",
            "  n033.txt\n",
            "  n028.txt\n",
            "  n048.txt\n",
            "  n056.txt\n",
            "  n072.txt\n",
            "  n069.txt\n",
            "  n049.txt\n",
            "  n091.txt\n",
            "  n013.txt\n",
            "  n050.txt\n",
            "  n067.txt\n",
            "  n073.txt\n",
            "  n020.txt\n",
            "  n065.txt\n",
            "  n001.txt\n",
            "  n059.txt\n",
            "  n093.txt\n",
            "  n046.txt\n",
            "  n025.txt\n",
            "  n023.txt\n",
            "  n022.txt\n",
            "  n053.txt\n",
            "  n064.txt\n",
            "  n005.txt\n",
            "  n074.txt\n",
            "  n086.txt\n",
            "  n014.txt\n",
            "  n063.txt\n",
            "  n080.txt\n",
            "  n095.txt\n",
            "  n075.txt\n",
            "  n088.txt\n",
            "  n071.txt\n",
            "  n066.txt\n",
            "  n098.txt\n",
            "  n034.txt\n",
            "  n042.txt\n",
            "  n043.txt\n",
            "  n054.txt\n",
            "  n094.txt\n",
            "  n100.txt\n",
            "  n099.txt\n",
            "  n026.txt\n",
            "  n060.txt\n",
            "  n004.txt\n",
            "  n061.txt\n",
            "  n010.txt\n",
            "  n030.txt\n",
            "  n003.txt\n",
            "  n029.txt\n",
            "  n057.txt\n",
            "  n032.txt\n",
            "  n039.txt\n",
            "  n077.txt\n",
            "  n038.txt\n",
            "  n031.txt\n",
            "  n037.txt\n",
            "  n045.txt\n",
            "  n096.txt\n",
            "  n052.txt\n",
            "  n009.txt\n",
            "  n040.txt\n",
            "  n002.txt\n",
            "  n078.txt\n",
            "  n018.txt\n",
            "  n007.txt\n",
            "  n083.txt\n",
            "  n041.txt\n",
            "  n024.txt\n",
            "  n082.txt\n",
            "  n058.txt\n",
            "  n015.txt\n",
            "  n051.txt\n",
            "  n021.txt\n",
            "  n011.txt\n",
            "  n084.txt\n",
            "  n070.txt\n",
            "  n068.txt\n",
            "  n008.txt\n",
            "  n019.txt\n",
            "  n035.txt\n",
            "  n062.txt\n",
            "  n016.txt\n",
            "  n044.txt\n",
            "  n089.txt\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEGDATA_EXTRACTED/S\n",
            "  S077.txt\n",
            "  S040.txt\n",
            "  S014.txt\n",
            "  S055.txt\n",
            "  S049.txt\n",
            "  S032.txt\n",
            "  S019.txt\n",
            "  S041.txt\n",
            "  S075.txt\n",
            "  S023.txt\n",
            "  S094.txt\n",
            "  S093.txt\n",
            "  S010.txt\n",
            "  S062.txt\n",
            "  S085.txt\n",
            "  S018.txt\n",
            "  S073.txt\n",
            "  S042.txt\n",
            "  S006.txt\n",
            "  S078.txt\n",
            "  S007.txt\n",
            "  S054.txt\n",
            "  S017.txt\n",
            "  S083.txt\n",
            "  S074.txt\n",
            "  S065.txt\n",
            "  S071.txt\n",
            "  S038.txt\n",
            "  S064.txt\n",
            "  S060.txt\n",
            "  S068.txt\n",
            "  S081.txt\n",
            "  S057.txt\n",
            "  S028.txt\n",
            "  S089.txt\n",
            "  S069.txt\n",
            "  S072.txt\n",
            "  S086.txt\n",
            "  S001.txt\n",
            "  S087.txt\n",
            "  S003.txt\n",
            "  S098.txt\n",
            "  S004.txt\n",
            "  S020.txt\n",
            "  S097.txt\n",
            "  S063.txt\n",
            "  S045.txt\n",
            "  S033.txt\n",
            "  S051.txt\n",
            "  S013.txt\n",
            "  S043.txt\n",
            "  S036.txt\n",
            "  S005.txt\n",
            "  S052.txt\n",
            "  S082.txt\n",
            "  S026.txt\n",
            "  S100.txt\n",
            "  S076.txt\n",
            "  S039.txt\n",
            "  S080.txt\n",
            "  S044.txt\n",
            "  S021.txt\n",
            "  S058.txt\n",
            "  S029.txt\n",
            "  S022.txt\n",
            "  S061.txt\n",
            "  S002.txt\n",
            "  S050.txt\n",
            "  S056.txt\n",
            "  S034.txt\n",
            "  S088.txt\n",
            "  S092.txt\n",
            "  S025.txt\n",
            "  S079.txt\n",
            "  S099.txt\n",
            "  S015.txt\n",
            "  S090.txt\n",
            "  S067.txt\n",
            "  S016.txt\n",
            "  S035.txt\n",
            "  S070.txt\n",
            "  S024.txt\n",
            "  S047.txt\n",
            "  S084.txt\n",
            "  S059.txt\n",
            "  S031.txt\n",
            "  S008.txt\n",
            "  S009.txt\n",
            "  S048.txt\n",
            "  S066.txt\n",
            "  S012.txt\n",
            "  S046.txt\n",
            "  S027.txt\n",
            "  S091.txt\n",
            "  S053.txt\n",
            "  S095.txt\n",
            "  S011.txt\n",
            "  S030.txt\n",
            "  S096.txt\n",
            "  S037.txt\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEGDATA_EXTRACTED/O\n",
            "  O061.txt\n",
            "  O050.txt\n",
            "  O060.txt\n",
            "  O057.txt\n",
            "  O031.txt\n",
            "  O047.txt\n",
            "  O100.txt\n",
            "  O044.txt\n",
            "  O026.txt\n",
            "  O062.txt\n",
            "  O095.txt\n",
            "  O046.txt\n",
            "  O076.txt\n",
            "  O075.txt\n",
            "  O011.txt\n",
            "  O059.txt\n",
            "  O066.txt\n",
            "  O094.txt\n",
            "  O004.txt\n",
            "  O014.txt\n",
            "  O001.txt\n",
            "  O038.txt\n",
            "  O099.txt\n",
            "  O008.txt\n",
            "  O021.txt\n",
            "  O012.txt\n",
            "  O077.txt\n",
            "  O030.txt\n",
            "  O022.txt\n",
            "  O045.txt\n",
            "  O063.txt\n",
            "  O018.txt\n",
            "  O088.txt\n",
            "  O051.txt\n",
            "  O019.txt\n",
            "  O056.txt\n",
            "  O086.txt\n",
            "  O072.txt\n",
            "  O091.txt\n",
            "  O055.txt\n",
            "  O025.txt\n",
            "  O041.txt\n",
            "  O071.txt\n",
            "  O033.txt\n",
            "  O052.txt\n",
            "  O007.txt\n",
            "  O083.txt\n",
            "  O023.txt\n",
            "  O081.txt\n",
            "  O079.txt\n",
            "  O067.txt\n",
            "  O082.txt\n",
            "  O005.txt\n",
            "  O034.txt\n",
            "  O040.txt\n",
            "  O084.txt\n",
            "  O037.txt\n",
            "  O039.txt\n",
            "  O028.txt\n",
            "  O092.txt\n",
            "  O003.txt\n",
            "  O017.txt\n",
            "  O043.txt\n",
            "  O054.txt\n",
            "  O016.txt\n",
            "  O078.txt\n",
            "  O029.txt\n",
            "  O069.txt\n",
            "  O065.txt\n",
            "  O093.txt\n",
            "  O087.txt\n",
            "  O064.txt\n",
            "  O049.txt\n",
            "  O096.txt\n",
            "  O058.txt\n",
            "  O089.txt\n",
            "  O048.txt\n",
            "  O015.txt\n",
            "  O097.txt\n",
            "  O070.txt\n",
            "  O053.txt\n",
            "  O032.txt\n",
            "  O073.txt\n",
            "  O098.txt\n",
            "  O085.txt\n",
            "  O035.txt\n",
            "  O036.txt\n",
            "  O013.txt\n",
            "  O002.txt\n",
            "  O020.txt\n",
            "  O027.txt\n",
            "  O042.txt\n",
            "  O080.txt\n",
            "  O009.txt\n",
            "  O024.txt\n",
            "  O074.txt\n",
            "  O090.txt\n",
            "  O006.txt\n",
            "  O010.txt\n",
            "  O068.txt\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEGDATA_EXTRACTED/Z\n",
            "  Z011.txt\n",
            "  Z078.txt\n",
            "  Z082.txt\n",
            "  Z002.txt\n",
            "  Z008.txt\n",
            "  Z046.txt\n",
            "  Z063.txt\n",
            "  Z045.txt\n",
            "  Z035.txt\n",
            "  Z032.txt\n",
            "  Z094.txt\n",
            "  Z003.txt\n",
            "  Z013.txt\n",
            "  Z096.txt\n",
            "  Z044.txt\n",
            "  Z004.txt\n",
            "  Z029.txt\n",
            "  Z074.txt\n",
            "  Z034.txt\n",
            "  Z021.txt\n",
            "  Z010.txt\n",
            "  Z057.txt\n",
            "  Z079.txt\n",
            "  Z023.txt\n",
            "  Z026.txt\n",
            "  Z097.txt\n",
            "  Z092.txt\n",
            "  Z081.txt\n",
            "  Z017.txt\n",
            "  Z006.txt\n",
            "  Z066.txt\n",
            "  Z015.txt\n",
            "  Z030.txt\n",
            "  Z041.txt\n",
            "  Z087.txt\n",
            "  Z019.txt\n",
            "  Z055.txt\n",
            "  Z095.txt\n",
            "  Z020.txt\n",
            "  Z007.txt\n",
            "  Z058.txt\n",
            "  Z090.txt\n",
            "  Z037.txt\n",
            "  Z080.txt\n",
            "  Z022.txt\n",
            "  Z028.txt\n",
            "  Z093.txt\n",
            "  Z086.txt\n",
            "  Z049.txt\n",
            "  Z014.txt\n",
            "  Z012.txt\n",
            "  Z005.txt\n",
            "  Z099.txt\n",
            "  Z062.txt\n",
            "  Z033.txt\n",
            "  Z050.txt\n",
            "  Z031.txt\n",
            "  Z073.txt\n",
            "  Z085.txt\n",
            "  Z076.txt\n",
            "  Z018.txt\n",
            "  Z016.txt\n",
            "  Z072.txt\n",
            "  Z025.txt\n",
            "  Z054.txt\n",
            "  Z027.txt\n",
            "  Z091.txt\n",
            "  Z043.txt\n",
            "  Z068.txt\n",
            "  Z064.txt\n",
            "  Z084.txt\n",
            "  Z038.txt\n",
            "  Z077.txt\n",
            "  Z070.txt\n",
            "  Z056.txt\n",
            "  Z100.txt\n",
            "  Z024.txt\n",
            "  Z040.txt\n",
            "  Z065.txt\n",
            "  Z009.txt\n",
            "  Z061.txt\n",
            "  Z039.txt\n",
            "  Z089.txt\n",
            "  Z098.txt\n",
            "  Z052.txt\n",
            "  Z067.txt\n",
            "  Z048.txt\n",
            "  Z036.txt\n",
            "  Z042.txt\n",
            "  Z047.txt\n",
            "  Z088.txt\n",
            "  Z075.txt\n",
            "  Z059.txt\n",
            "  Z053.txt\n",
            "  Z060.txt\n",
            "  Z083.txt\n",
            "  Z069.txt\n",
            "  Z001.txt\n",
            "  Z071.txt\n",
            "  Z051.txt\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEGDATA_EXTRACTED/F\n",
            "  F061.txt\n",
            "  F075.txt\n",
            "  F094.txt\n",
            "  F017.txt\n",
            "  F022.txt\n",
            "  F057.txt\n",
            "  F063.txt\n",
            "  F064.txt\n",
            "  F028.txt\n",
            "  F090.txt\n",
            "  F040.txt\n",
            "  F032.txt\n",
            "  F054.txt\n",
            "  F079.txt\n",
            "  F080.txt\n",
            "  F066.txt\n",
            "  F069.txt\n",
            "  F059.txt\n",
            "  F044.txt\n",
            "  F011.txt\n",
            "  F042.txt\n",
            "  F083.txt\n",
            "  F068.txt\n",
            "  F010.txt\n",
            "  F072.txt\n",
            "  F024.txt\n",
            "  F019.txt\n",
            "  F065.txt\n",
            "  F006.txt\n",
            "  F023.txt\n",
            "  F078.txt\n",
            "  F093.txt\n",
            "  F058.txt\n",
            "  F031.txt\n",
            "  F035.txt\n",
            "  F030.txt\n",
            "  F036.txt\n",
            "  F015.txt\n",
            "  F100.txt\n",
            "  F018.txt\n",
            "  F070.txt\n",
            "  F052.txt\n",
            "  F074.txt\n",
            "  F025.txt\n",
            "  F008.txt\n",
            "  F089.txt\n",
            "  F098.txt\n",
            "  F082.txt\n",
            "  F091.txt\n",
            "  F081.txt\n",
            "  F020.txt\n",
            "  F037.txt\n",
            "  F096.txt\n",
            "  F005.txt\n",
            "  F099.txt\n",
            "  F051.txt\n",
            "  F056.txt\n",
            "  F039.txt\n",
            "  F067.txt\n",
            "  F077.txt\n",
            "  F053.txt\n",
            "  F041.txt\n",
            "  F097.txt\n",
            "  F055.txt\n",
            "  F086.txt\n",
            "  F002.txt\n",
            "  F062.txt\n",
            "  F007.txt\n",
            "  F043.txt\n",
            "  F021.txt\n",
            "  F003.txt\n",
            "  F029.txt\n",
            "  F085.txt\n",
            "  F046.txt\n",
            "  F088.txt\n",
            "  F087.txt\n",
            "  F038.txt\n",
            "  F092.txt\n",
            "  F004.txt\n",
            "  F009.txt\n",
            "  F034.txt\n",
            "  F033.txt\n",
            "  F073.txt\n",
            "  F001.txt\n",
            "  F013.txt\n",
            "  F045.txt\n",
            "  F084.txt\n",
            "  F060.txt\n",
            "  F016.txt\n",
            "  F071.txt\n",
            "  F014.txt\n",
            "  F050.txt\n",
            "  F026.txt\n",
            "  F027.txt\n",
            "  F047.txt\n",
            "  F095.txt\n",
            "  F049.txt\n",
            "  F076.txt\n",
            "  F048.txt\n",
            "  F012.txt\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEG_METRICS\n",
            "  validation_metrics.csv\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEG_METRICS/Z\n",
            "  Z039.npz\n",
            "  Z089.npz\n",
            "  Z083.npz\n",
            "  Z072.npz\n",
            "  Z062.npz\n",
            "  Z024.npz\n",
            "  Z002.npz\n",
            "  Z018.npz\n",
            "  Z035.npz\n",
            "  Z097.npz\n",
            "  Z046.npz\n",
            "  Z007.npz\n",
            "  Z054.npz\n",
            "  Z084.npz\n",
            "  Z056.npz\n",
            "  Z020.npz\n",
            "  Z077.npz\n",
            "  Z100.npz\n",
            "  Z038.npz\n",
            "  Z040.npz\n",
            "  Z047.npz\n",
            "  Z073.npz\n",
            "  Z027.npz\n",
            "  Z081.npz\n",
            "  Z017.npz\n",
            "  Z022.npz\n",
            "  Z096.npz\n",
            "  Z006.npz\n",
            "  Z051.npz\n",
            "  Z061.npz\n",
            "  Z082.npz\n",
            "  Z087.npz\n",
            "  Z069.npz\n",
            "  Z019.npz\n",
            "  Z026.npz\n",
            "  Z031.npz\n",
            "  Z012.npz\n",
            "  Z064.npz\n",
            "  Z099.npz\n",
            "  Z080.npz\n",
            "  Z058.npz\n",
            "  Z050.npz\n",
            "  Z093.npz\n",
            "  Z042.npz\n",
            "  Z067.npz\n",
            "  Z078.npz\n",
            "  Z044.npz\n",
            "  Z060.npz\n",
            "  Z055.npz\n",
            "  Z063.npz\n",
            "  Z065.npz\n",
            "  Z074.npz\n",
            "  Z085.npz\n",
            "  Z098.npz\n",
            "  Z010.npz\n",
            "  Z011.npz\n",
            "  Z037.npz\n",
            "  Z052.npz\n",
            "  Z076.npz\n",
            "  Z049.npz\n",
            "  Z053.npz\n",
            "  Z070.npz\n",
            "  Z045.npz\n",
            "  Z092.npz\n",
            "  Z090.npz\n",
            "  Z033.npz\n",
            "  Z048.npz\n",
            "  Z088.npz\n",
            "  Z095.npz\n",
            "  Z023.npz\n",
            "  Z001.npz\n",
            "  Z091.npz\n",
            "  Z003.npz\n",
            "  Z030.npz\n",
            "  Z041.npz\n",
            "  Z094.npz\n",
            "  Z066.npz\n",
            "  Z009.npz\n",
            "  Z034.npz\n",
            "  Z016.npz\n",
            "  Z086.npz\n",
            "  Z071.npz\n",
            "  Z028.npz\n",
            "  Z075.npz\n",
            "  Z032.npz\n",
            "  Z036.npz\n",
            "  Z013.npz\n",
            "  Z014.npz\n",
            "  Z015.npz\n",
            "  Z025.npz\n",
            "  Z043.npz\n",
            "  Z059.npz\n",
            "  Z008.npz\n",
            "  Z021.npz\n",
            "  Z004.npz\n",
            "  Z068.npz\n",
            "  Z029.npz\n",
            "  Z057.npz\n",
            "  Z005.npz\n",
            "  Z079.npz\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEG_METRICS/S\n",
            "  S072.npz\n",
            "  S029.npz\n",
            "  S050.npz\n",
            "  S068.npz\n",
            "  S060.npz\n",
            "  S048.npz\n",
            "  S037.npz\n",
            "  S070.npz\n",
            "  S057.npz\n",
            "  S094.npz\n",
            "  S040.npz\n",
            "  S030.npz\n",
            "  S046.npz\n",
            "  S025.npz\n",
            "  S035.npz\n",
            "  S089.npz\n",
            "  S063.npz\n",
            "  S022.npz\n",
            "  S095.npz\n",
            "  S084.npz\n",
            "  S036.npz\n",
            "  S069.npz\n",
            "  S059.npz\n",
            "  S004.npz\n",
            "  S064.npz\n",
            "  S090.npz\n",
            "  S071.npz\n",
            "  S008.npz\n",
            "  S073.npz\n",
            "  S078.npz\n",
            "  S042.npz\n",
            "  S074.npz\n",
            "  S019.npz\n",
            "  S058.npz\n",
            "  S007.npz\n",
            "  S066.npz\n",
            "  S023.npz\n",
            "  S080.npz\n",
            "  S012.npz\n",
            "  S041.npz\n",
            "  S043.npz\n",
            "  S096.npz\n",
            "  S087.npz\n",
            "  S081.npz\n",
            "  S044.npz\n",
            "  S027.npz\n",
            "  S088.npz\n",
            "  S053.npz\n",
            "  S002.npz\n",
            "  S079.npz\n",
            "  S098.npz\n",
            "  S052.npz\n",
            "  S083.npz\n",
            "  S045.npz\n",
            "  S092.npz\n",
            "  S018.npz\n",
            "  S001.npz\n",
            "  S038.npz\n",
            "  S085.npz\n",
            "  S003.npz\n",
            "  S009.npz\n",
            "  S047.npz\n",
            "  S077.npz\n",
            "  S024.npz\n",
            "  S055.npz\n",
            "  S039.npz\n",
            "  S062.npz\n",
            "  S017.npz\n",
            "  S006.npz\n",
            "  S065.npz\n",
            "  S011.npz\n",
            "  S026.npz\n",
            "  S097.npz\n",
            "  S100.npz\n",
            "  S015.npz\n",
            "  S034.npz\n",
            "  S086.npz\n",
            "  S031.npz\n",
            "  S054.npz\n",
            "  S076.npz\n",
            "  S005.npz\n",
            "  S010.npz\n",
            "  S075.npz\n",
            "  S013.npz\n",
            "  S056.npz\n",
            "  S016.npz\n",
            "  S051.npz\n",
            "  S049.npz\n",
            "  S099.npz\n",
            "  S020.npz\n",
            "  S067.npz\n",
            "  S061.npz\n",
            "  S033.npz\n",
            "  S028.npz\n",
            "  S082.npz\n",
            "  S091.npz\n",
            "  S021.npz\n",
            "  S014.npz\n",
            "  S032.npz\n",
            "  S093.npz\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEG_METRICS/O\n",
            "  O091.npz\n",
            "  O033.npz\n",
            "  O043.npz\n",
            "  O034.npz\n",
            "  O075.npz\n",
            "  O093.npz\n",
            "  O098.npz\n",
            "  O036.npz\n",
            "  O073.npz\n",
            "  O012.npz\n",
            "  O063.npz\n",
            "  O021.npz\n",
            "  O005.npz\n",
            "  O042.npz\n",
            "  O030.npz\n",
            "  O057.npz\n",
            "  O081.npz\n",
            "  O009.npz\n",
            "  O019.npz\n",
            "  O100.npz\n",
            "  O074.npz\n",
            "  O010.npz\n",
            "  O099.npz\n",
            "  O001.npz\n",
            "  O025.npz\n",
            "  O040.npz\n",
            "  O084.npz\n",
            "  O083.npz\n",
            "  O079.npz\n",
            "  O062.npz\n",
            "  O006.npz\n",
            "  O029.npz\n",
            "  O038.npz\n",
            "  O052.npz\n",
            "  O055.npz\n",
            "  O003.npz\n",
            "  O015.npz\n",
            "  O085.npz\n",
            "  O059.npz\n",
            "  O068.npz\n",
            "  O061.npz\n",
            "  O017.npz\n",
            "  O035.npz\n",
            "  O095.npz\n",
            "  O089.npz\n",
            "  O027.npz\n",
            "  O076.npz\n",
            "  O096.npz\n",
            "  O050.npz\n",
            "  O069.npz\n",
            "  O007.npz\n",
            "  O092.npz\n",
            "  O024.npz\n",
            "  O072.npz\n",
            "  O060.npz\n",
            "  O086.npz\n",
            "  O047.npz\n",
            "  O044.npz\n",
            "  O028.npz\n",
            "  O067.npz\n",
            "  O002.npz\n",
            "  O016.npz\n",
            "  O046.npz\n",
            "  O023.npz\n",
            "  O045.npz\n",
            "  O080.npz\n",
            "  O058.npz\n",
            "  O064.npz\n",
            "  O049.npz\n",
            "  O090.npz\n",
            "  O004.npz\n",
            "  O041.npz\n",
            "  O082.npz\n",
            "  O094.npz\n",
            "  O020.npz\n",
            "  O078.npz\n",
            "  O014.npz\n",
            "  O037.npz\n",
            "  O053.npz\n",
            "  O039.npz\n",
            "  O097.npz\n",
            "  O031.npz\n",
            "  O018.npz\n",
            "  O008.npz\n",
            "  O087.npz\n",
            "  O032.npz\n",
            "  O051.npz\n",
            "  O022.npz\n",
            "  O013.npz\n",
            "  O088.npz\n",
            "  O056.npz\n",
            "  O054.npz\n",
            "  O048.npz\n",
            "  O071.npz\n",
            "  O066.npz\n",
            "  O077.npz\n",
            "  O065.npz\n",
            "  O026.npz\n",
            "  O011.npz\n",
            "  O070.npz\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEG_METRICS/N\n",
            "  n031.npz\n",
            "  n002.npz\n",
            "  n065.npz\n",
            "  n041.npz\n",
            "  n071.npz\n",
            "  n086.npz\n",
            "  n087.npz\n",
            "  n078.npz\n",
            "  n013.npz\n",
            "  n008.npz\n",
            "  n097.npz\n",
            "  n045.npz\n",
            "  n038.npz\n",
            "  n099.npz\n",
            "  n029.npz\n",
            "  n084.npz\n",
            "  n027.npz\n",
            "  n092.npz\n",
            "  n079.npz\n",
            "  n032.npz\n",
            "  n018.npz\n",
            "  n085.npz\n",
            "  n021.npz\n",
            "  n003.npz\n",
            "  n049.npz\n",
            "  n056.npz\n",
            "  n016.npz\n",
            "  n053.npz\n",
            "  n005.npz\n",
            "  n073.npz\n",
            "  n026.npz\n",
            "  n075.npz\n",
            "  n023.npz\n",
            "  n067.npz\n",
            "  n022.npz\n",
            "  n042.npz\n",
            "  n080.npz\n",
            "  n091.npz\n",
            "  n057.npz\n",
            "  n089.npz\n",
            "  n076.npz\n",
            "  n064.npz\n",
            "  n011.npz\n",
            "  n019.npz\n",
            "  n098.npz\n",
            "  n095.npz\n",
            "  n068.npz\n",
            "  n094.npz\n",
            "  n081.npz\n",
            "  n050.npz\n",
            "  n046.npz\n",
            "  n070.npz\n",
            "  n083.npz\n",
            "  n058.npz\n",
            "  n063.npz\n",
            "  n037.npz\n",
            "  n051.npz\n",
            "  n047.npz\n",
            "  n034.npz\n",
            "  n052.npz\n",
            "  n006.npz\n",
            "  n044.npz\n",
            "  n024.npz\n",
            "  n001.npz\n",
            "  n012.npz\n",
            "  n035.npz\n",
            "  n010.npz\n",
            "  n025.npz\n",
            "  n074.npz\n",
            "  n055.npz\n",
            "  n093.npz\n",
            "  n004.npz\n",
            "  n062.npz\n",
            "  n040.npz\n",
            "  n072.npz\n",
            "  n009.npz\n",
            "  n043.npz\n",
            "  n028.npz\n",
            "  n066.npz\n",
            "  n060.npz\n",
            "  n077.npz\n",
            "  n007.npz\n",
            "  n054.npz\n",
            "  n036.npz\n",
            "  n039.npz\n",
            "  n096.npz\n",
            "  n033.npz\n",
            "  n020.npz\n",
            "  n048.npz\n",
            "  n061.npz\n",
            "  n015.npz\n",
            "  n100.npz\n",
            "  n090.npz\n",
            "  n030.npz\n",
            "  n017.npz\n",
            "  n014.npz\n",
            "  n059.npz\n",
            "  n069.npz\n",
            "  n088.npz\n",
            "  n082.npz\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEG_METRICS/F\n",
            "  F024.npz\n",
            "  F023.npz\n",
            "  F007.npz\n",
            "  F026.npz\n",
            "  F008.npz\n",
            "  F072.npz\n",
            "  F064.npz\n",
            "  F020.npz\n",
            "  F029.npz\n",
            "  F032.npz\n",
            "  F069.npz\n",
            "  F075.npz\n",
            "  F094.npz\n",
            "  F073.npz\n",
            "  F056.npz\n",
            "  F002.npz\n",
            "  F033.npz\n",
            "  F046.npz\n",
            "  F098.npz\n",
            "  F049.npz\n",
            "  F079.npz\n",
            "  F057.npz\n",
            "  F018.npz\n",
            "  F044.npz\n",
            "  F090.npz\n",
            "  F038.npz\n",
            "  F003.npz\n",
            "  F062.npz\n",
            "  F088.npz\n",
            "  F015.npz\n",
            "  F074.npz\n",
            "  F021.npz\n",
            "  F028.npz\n",
            "  F054.npz\n",
            "  F092.npz\n",
            "  F031.npz\n",
            "  F047.npz\n",
            "  F078.npz\n",
            "  F059.npz\n",
            "  F067.npz\n",
            "  F013.npz\n",
            "  F063.npz\n",
            "  F071.npz\n",
            "  F065.npz\n",
            "  F014.npz\n",
            "  F061.npz\n",
            "  F048.npz\n",
            "  F041.npz\n",
            "  F081.npz\n",
            "  F091.npz\n",
            "  F001.npz\n",
            "  F036.npz\n",
            "  F084.npz\n",
            "  F058.npz\n",
            "  F066.npz\n",
            "  F089.npz\n",
            "  F076.npz\n",
            "  F060.npz\n",
            "  F068.npz\n",
            "  F100.npz\n",
            "  F034.npz\n",
            "  F006.npz\n",
            "  F053.npz\n",
            "  F004.npz\n",
            "  F093.npz\n",
            "  F025.npz\n",
            "  F009.npz\n",
            "  F045.npz\n",
            "  F037.npz\n",
            "  F085.npz\n",
            "  F012.npz\n",
            "  F027.npz\n",
            "  F080.npz\n",
            "  F055.npz\n",
            "  F016.npz\n",
            "  F087.npz\n",
            "  F052.npz\n",
            "  F070.npz\n",
            "  F096.npz\n",
            "  F086.npz\n",
            "  F035.npz\n",
            "  F099.npz\n",
            "  F019.npz\n",
            "  F039.npz\n",
            "  F050.npz\n",
            "  F077.npz\n",
            "  F040.npz\n",
            "  F095.npz\n",
            "  F030.npz\n",
            "  F051.npz\n",
            "  F011.npz\n",
            "  F022.npz\n",
            "  F082.npz\n",
            "  F083.npz\n",
            "  F097.npz\n",
            "  F043.npz\n",
            "  F010.npz\n",
            "  F017.npz\n",
            "  F005.npz\n",
            "  F042.npz\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEGDATA_FILTERED\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEGDATA_FILTERED/Z\n",
            "  Z090.txt\n",
            "  Z026.txt\n",
            "  Z099.txt\n",
            "  Z098.txt\n",
            "  Z050.txt\n",
            "  Z063.txt\n",
            "  Z025.txt\n",
            "  Z077.txt\n",
            "  Z038.txt\n",
            "  Z031.txt\n",
            "  Z064.txt\n",
            "  Z032.txt\n",
            "  Z024.txt\n",
            "  Z083.txt\n",
            "  Z028.txt\n",
            "  Z047.txt\n",
            "  Z036.txt\n",
            "  Z094.txt\n",
            "  Z069.txt\n",
            "  Z001.txt\n",
            "  Z055.txt\n",
            "  Z043.txt\n",
            "  Z029.txt\n",
            "  Z003.txt\n",
            "  Z096.txt\n",
            "  Z062.txt\n",
            "  Z071.txt\n",
            "  Z086.txt\n",
            "  Z082.txt\n",
            "  Z074.txt\n",
            "  Z023.txt\n",
            "  Z075.txt\n",
            "  Z037.txt\n",
            "  Z073.txt\n",
            "  Z078.txt\n",
            "  Z013.txt\n",
            "  Z058.txt\n",
            "  Z044.txt\n",
            "  Z095.txt\n",
            "  Z008.txt\n",
            "  Z089.txt\n",
            "  Z091.txt\n",
            "  Z015.txt\n",
            "  Z057.txt\n",
            "  Z052.txt\n",
            "  Z068.txt\n",
            "  Z048.txt\n",
            "  Z053.txt\n",
            "  Z072.txt\n",
            "  Z079.txt\n",
            "  Z033.txt\n",
            "  Z039.txt\n",
            "  Z009.txt\n",
            "  Z065.txt\n",
            "  Z042.txt\n",
            "  Z027.txt\n",
            "  Z060.txt\n",
            "  Z059.txt\n",
            "  Z097.txt\n",
            "  Z016.txt\n",
            "  Z012.txt\n",
            "  Z034.txt\n",
            "  Z021.txt\n",
            "  Z046.txt\n",
            "  Z085.txt\n",
            "  Z002.txt\n",
            "  Z051.txt\n",
            "  Z076.txt\n",
            "  Z081.txt\n",
            "  Z006.txt\n",
            "  Z017.txt\n",
            "  Z022.txt\n",
            "  Z040.txt\n",
            "  Z088.txt\n",
            "  Z080.txt\n",
            "  Z007.txt\n",
            "  Z011.txt\n",
            "  Z018.txt\n",
            "  Z061.txt\n",
            "  Z010.txt\n",
            "  Z030.txt\n",
            "  Z041.txt\n",
            "  Z045.txt\n",
            "  Z049.txt\n",
            "  Z100.txt\n",
            "  Z087.txt\n",
            "  Z093.txt\n",
            "  Z070.txt\n",
            "  Z056.txt\n",
            "  Z054.txt\n",
            "  Z019.txt\n",
            "  Z084.txt\n",
            "  Z020.txt\n",
            "  Z014.txt\n",
            "  Z035.txt\n",
            "  Z067.txt\n",
            "  Z004.txt\n",
            "  Z066.txt\n",
            "  Z005.txt\n",
            "  Z092.txt\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEGDATA_FILTERED/N\n",
            "  n082.txt\n",
            "  n088.txt\n",
            "  n067.txt\n",
            "  n063.txt\n",
            "  n060.txt\n",
            "  n057.txt\n",
            "  n030.txt\n",
            "  n010.txt\n",
            "  n092.txt\n",
            "  n027.txt\n",
            "  n032.txt\n",
            "  n029.txt\n",
            "  n019.txt\n",
            "  n086.txt\n",
            "  n002.txt\n",
            "  n003.txt\n",
            "  n038.txt\n",
            "  n043.txt\n",
            "  n014.txt\n",
            "  n012.txt\n",
            "  n016.txt\n",
            "  n100.txt\n",
            "  n096.txt\n",
            "  n078.txt\n",
            "  n084.txt\n",
            "  n095.txt\n",
            "  n026.txt\n",
            "  n068.txt\n",
            "  n020.txt\n",
            "  n054.txt\n",
            "  n049.txt\n",
            "  n064.txt\n",
            "  n017.txt\n",
            "  n077.txt\n",
            "  n050.txt\n",
            "  n013.txt\n",
            "  n025.txt\n",
            "  n009.txt\n",
            "  n028.txt\n",
            "  n066.txt\n",
            "  n007.txt\n",
            "  n039.txt\n",
            "  n045.txt\n",
            "  n036.txt\n",
            "  n046.txt\n",
            "  n072.txt\n",
            "  n097.txt\n",
            "  n083.txt\n",
            "  n001.txt\n",
            "  n076.txt\n",
            "  n061.txt\n",
            "  n089.txt\n",
            "  n015.txt\n",
            "  n071.txt\n",
            "  n034.txt\n",
            "  n085.txt\n",
            "  n023.txt\n",
            "  n069.txt\n",
            "  n098.txt\n",
            "  n090.txt\n",
            "  n065.txt\n",
            "  n005.txt\n",
            "  n040.txt\n",
            "  n021.txt\n",
            "  n062.txt\n",
            "  n006.txt\n",
            "  n042.txt\n",
            "  n011.txt\n",
            "  n087.txt\n",
            "  n070.txt\n",
            "  n037.txt\n",
            "  n075.txt\n",
            "  n093.txt\n",
            "  n024.txt\n",
            "  n008.txt\n",
            "  n073.txt\n",
            "  n058.txt\n",
            "  n056.txt\n",
            "  n091.txt\n",
            "  n079.txt\n",
            "  n051.txt\n",
            "  n059.txt\n",
            "  n052.txt\n",
            "  n080.txt\n",
            "  n031.txt\n",
            "  n044.txt\n",
            "  n099.txt\n",
            "  n033.txt\n",
            "  n004.txt\n",
            "  n041.txt\n",
            "  n053.txt\n",
            "  n074.txt\n",
            "  n048.txt\n",
            "  n047.txt\n",
            "  n018.txt\n",
            "  n022.txt\n",
            "  n035.txt\n",
            "  n055.txt\n",
            "  n081.txt\n",
            "  n094.txt\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEGDATA_FILTERED/S\n",
            "  S036.txt\n",
            "  S094.txt\n",
            "  S037.txt\n",
            "  S019.txt\n",
            "  S041.txt\n",
            "  S042.txt\n",
            "  S099.txt\n",
            "  S022.txt\n",
            "  S012.txt\n",
            "  S085.txt\n",
            "  S002.txt\n",
            "  S028.txt\n",
            "  S090.txt\n",
            "  S065.txt\n",
            "  S003.txt\n",
            "  S066.txt\n",
            "  S011.txt\n",
            "  S078.txt\n",
            "  S038.txt\n",
            "  S097.txt\n",
            "  S063.txt\n",
            "  S059.txt\n",
            "  S014.txt\n",
            "  S046.txt\n",
            "  S008.txt\n",
            "  S015.txt\n",
            "  S086.txt\n",
            "  S093.txt\n",
            "  S056.txt\n",
            "  S025.txt\n",
            "  S001.txt\n",
            "  S083.txt\n",
            "  S075.txt\n",
            "  S032.txt\n",
            "  S069.txt\n",
            "  S029.txt\n",
            "  S018.txt\n",
            "  S088.txt\n",
            "  S055.txt\n",
            "  S082.txt\n",
            "  S098.txt\n",
            "  S062.txt\n",
            "  S026.txt\n",
            "  S081.txt\n",
            "  S070.txt\n",
            "  S045.txt\n",
            "  S020.txt\n",
            "  S024.txt\n",
            "  S051.txt\n",
            "  S052.txt\n",
            "  S040.txt\n",
            "  S058.txt\n",
            "  S068.txt\n",
            "  S049.txt\n",
            "  S053.txt\n",
            "  S030.txt\n",
            "  S092.txt\n",
            "  S060.txt\n",
            "  S096.txt\n",
            "  S047.txt\n",
            "  S089.txt\n",
            "  S077.txt\n",
            "  S095.txt\n",
            "  S043.txt\n",
            "  S039.txt\n",
            "  S076.txt\n",
            "  S091.txt\n",
            "  S100.txt\n",
            "  S044.txt\n",
            "  S057.txt\n",
            "  S004.txt\n",
            "  S071.txt\n",
            "  S073.txt\n",
            "  S087.txt\n",
            "  S050.txt\n",
            "  S009.txt\n",
            "  S048.txt\n",
            "  S035.txt\n",
            "  S084.txt\n",
            "  S033.txt\n",
            "  S007.txt\n",
            "  S067.txt\n",
            "  S080.txt\n",
            "  S010.txt\n",
            "  S016.txt\n",
            "  S034.txt\n",
            "  S079.txt\n",
            "  S006.txt\n",
            "  S005.txt\n",
            "  S054.txt\n",
            "  S074.txt\n",
            "  S064.txt\n",
            "  S072.txt\n",
            "  S061.txt\n",
            "  S017.txt\n",
            "  S027.txt\n",
            "  S031.txt\n",
            "  S013.txt\n",
            "  S021.txt\n",
            "  S023.txt\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEGDATA_FILTERED/O\n",
            "  O087.txt\n",
            "  O048.txt\n",
            "  O100.txt\n",
            "  O005.txt\n",
            "  O038.txt\n",
            "  O034.txt\n",
            "  O093.txt\n",
            "  O020.txt\n",
            "  O009.txt\n",
            "  O012.txt\n",
            "  O046.txt\n",
            "  O098.txt\n",
            "  O050.txt\n",
            "  O061.txt\n",
            "  O077.txt\n",
            "  O081.txt\n",
            "  O065.txt\n",
            "  O041.txt\n",
            "  O035.txt\n",
            "  O096.txt\n",
            "  O044.txt\n",
            "  O069.txt\n",
            "  O010.txt\n",
            "  O030.txt\n",
            "  O071.txt\n",
            "  O060.txt\n",
            "  O019.txt\n",
            "  O099.txt\n",
            "  O073.txt\n",
            "  O003.txt\n",
            "  O016.txt\n",
            "  O036.txt\n",
            "  O068.txt\n",
            "  O047.txt\n",
            "  O040.txt\n",
            "  O037.txt\n",
            "  O029.txt\n",
            "  O075.txt\n",
            "  O017.txt\n",
            "  O090.txt\n",
            "  O033.txt\n",
            "  O032.txt\n",
            "  O056.txt\n",
            "  O062.txt\n",
            "  O039.txt\n",
            "  O045.txt\n",
            "  O067.txt\n",
            "  O022.txt\n",
            "  O064.txt\n",
            "  O006.txt\n",
            "  O085.txt\n",
            "  O097.txt\n",
            "  O059.txt\n",
            "  O072.txt\n",
            "  O095.txt\n",
            "  O092.txt\n",
            "  O007.txt\n",
            "  O058.txt\n",
            "  O008.txt\n",
            "  O004.txt\n",
            "  O015.txt\n",
            "  O079.txt\n",
            "  O089.txt\n",
            "  O088.txt\n",
            "  O024.txt\n",
            "  O049.txt\n",
            "  O054.txt\n",
            "  O025.txt\n",
            "  O066.txt\n",
            "  O011.txt\n",
            "  O057.txt\n",
            "  O082.txt\n",
            "  O078.txt\n",
            "  O018.txt\n",
            "  O028.txt\n",
            "  O094.txt\n",
            "  O070.txt\n",
            "  O086.txt\n",
            "  O053.txt\n",
            "  O091.txt\n",
            "  O043.txt\n",
            "  O051.txt\n",
            "  O021.txt\n",
            "  O042.txt\n",
            "  O080.txt\n",
            "  O052.txt\n",
            "  O026.txt\n",
            "  O084.txt\n",
            "  O027.txt\n",
            "  O074.txt\n",
            "  O002.txt\n",
            "  O063.txt\n",
            "  O014.txt\n",
            "  O055.txt\n",
            "  O031.txt\n",
            "  O076.txt\n",
            "  O013.txt\n",
            "  O023.txt\n",
            "  O001.txt\n",
            "  O083.txt\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEGDATA_FILTERED/F\n",
            "  F067.txt\n",
            "  F041.txt\n",
            "  F052.txt\n",
            "  F086.txt\n",
            "  F098.txt\n",
            "  F068.txt\n",
            "  F007.txt\n",
            "  F037.txt\n",
            "  F077.txt\n",
            "  F038.txt\n",
            "  F048.txt\n",
            "  F032.txt\n",
            "  F023.txt\n",
            "  F035.txt\n",
            "  F003.txt\n",
            "  F028.txt\n",
            "  F029.txt\n",
            "  F046.txt\n",
            "  F043.txt\n",
            "  F024.txt\n",
            "  F025.txt\n",
            "  F047.txt\n",
            "  F020.txt\n",
            "  F095.txt\n",
            "  F008.txt\n",
            "  F013.txt\n",
            "  F071.txt\n",
            "  F060.txt\n",
            "  F079.txt\n",
            "  F087.txt\n",
            "  F036.txt\n",
            "  F016.txt\n",
            "  F081.txt\n",
            "  F096.txt\n",
            "  F061.txt\n",
            "  F094.txt\n",
            "  F053.txt\n",
            "  F049.txt\n",
            "  F074.txt\n",
            "  F006.txt\n",
            "  F069.txt\n",
            "  F090.txt\n",
            "  F100.txt\n",
            "  F062.txt\n",
            "  F011.txt\n",
            "  F030.txt\n",
            "  F093.txt\n",
            "  F091.txt\n",
            "  F044.txt\n",
            "  F019.txt\n",
            "  F055.txt\n",
            "  F027.txt\n",
            "  F031.txt\n",
            "  F015.txt\n",
            "  F056.txt\n",
            "  F057.txt\n",
            "  F042.txt\n",
            "  F039.txt\n",
            "  F092.txt\n",
            "  F012.txt\n",
            "  F059.txt\n",
            "  F051.txt\n",
            "  F089.txt\n",
            "  F075.txt\n",
            "  F017.txt\n",
            "  F040.txt\n",
            "  F070.txt\n",
            "  F082.txt\n",
            "  F045.txt\n",
            "  F076.txt\n",
            "  F034.txt\n",
            "  F064.txt\n",
            "  F058.txt\n",
            "  F066.txt\n",
            "  F063.txt\n",
            "  F097.txt\n",
            "  F085.txt\n",
            "  F010.txt\n",
            "  F083.txt\n",
            "  F026.txt\n",
            "  F004.txt\n",
            "  F072.txt\n",
            "  F009.txt\n",
            "  F002.txt\n",
            "  F033.txt\n",
            "  F054.txt\n",
            "  F018.txt\n",
            "  F065.txt\n",
            "  F078.txt\n",
            "  F022.txt\n",
            "  F014.txt\n",
            "  F080.txt\n",
            "  F001.txt\n",
            "  F088.txt\n",
            "  F050.txt\n",
            "  F005.txt\n",
            "  F099.txt\n",
            "  F084.txt\n",
            "  F021.txt\n",
            "  F073.txt\n",
            "Directory: /content/drive/MyDrive/EEG_DATA_REPO/EEGDATA\n",
            "  S.zip\n",
            "  Z.zip\n",
            "  O.zip\n",
            "  N.zip\n",
            "  F.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN starts here\n"
      ],
      "metadata": {
        "id": "-dLcGGhue-Ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch with CUDA support (Colab default is usually CUDA 11.x)\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "I-ij6SyLgI8s",
        "outputId": "e474cf9a-168e-443e-d908-c88913b94b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (780.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.5/780.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.1.105 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/EEG_DATA_REPO'\n",
        "print(os.listdir(data_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWSvlqkzioJw",
        "outputId": "25297212-361c-4946-e106-e3f142becad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['seizure_state_comparison.png', 'SeizureDetectionCode.ipynb', 'SeizDetect.ipynb', 'EEGDATA_EXTRACTED', 'EEG_METRICS', 'EEGDATA_FILTERED', 'EEGDATA']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "npz = np.load('/content/drive/MyDrive/EEG_DATA_REPO/EEG_METRICS/Z/Z039.npz')\n",
        "print(npz.files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MwYsPgVhQgb",
        "outputId": "0e36d6ff-830a-484f-f3d5-d710ddd6eeeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['name', 'stationarity', 'd2_real', 'd2_surr_mean', 'd2_surr_std', 'nle', 'basic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# --------------------\n",
        "# Dataset Loader\n",
        "# --------------------\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, data_root, fixed_length=4096):\n",
        "        self.files = []\n",
        "        self.labels = []\n",
        "        self.data_root = data_root\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "        # Map folder name to integer labels\n",
        "        label_map = {'F':0, 'N':1, 'O':2, 'S':3, 'Z':4}\n",
        "\n",
        "        # Directory containing filtered EEG data\n",
        "        filtered_data_dir = os.path.join(data_root, 'EEGDATA_FILTERED')\n",
        "\n",
        "        for folder_name, label in label_map.items():\n",
        "            folder_path = os.path.join(filtered_data_dir, folder_name)\n",
        "            if os.path.isdir(folder_path):\n",
        "                for file in os.listdir(folder_path):\n",
        "                    if file.endswith(\".txt\") or file.endswith(\"_filtered.txt\"):\n",
        "                        self.files.append(os.path.join(folder_path, file))\n",
        "                        self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        signal = np.loadtxt(self.files[idx])\n",
        "\n",
        "        # Pad or truncate\n",
        "        if len(signal) < self.fixed_length:\n",
        "            signal = np.pad(signal, (0, self.fixed_length - len(signal)), 'constant')\n",
        "        else:\n",
        "            signal = signal[:self.fixed_length]\n",
        "\n",
        "        # Standardize signal\n",
        "        signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-8)\n",
        "\n",
        "        signal = torch.tensor(signal, dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return signal, label\n",
        "\n",
        "# --------------------\n",
        "# Improved CNN\n",
        "# --------------------\n",
        "class ImprovedEEGCNN(nn.Module):\n",
        "    def __init__(self, input_channels=1, num_classes=5):\n",
        "        super(ImprovedEEGCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = torch.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# --------------------\n",
        "# DataLoader\n",
        "# --------------------\n",
        "data_root = '/content/drive/MyDrive/EEG_DATA_REPO'\n",
        "dataset = EEGDataset(data_root)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "# --------------------\n",
        "# Training loop\n",
        "# --------------------\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = ImprovedEEGCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(20):  # train longer\n",
        "    model.train()\n",
        "    for x, y in train_loader:\n",
        "        x = x.unsqueeze(1).to(device)  # add channel dim\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1} done\")\n"
      ],
      "metadata": {
        "id": "aGj_8uv8mlZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "        x = x.unsqueeze(1).to(device)\n",
        "        y = y.to(device)\n",
        "        out = model(x)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "print(\"Validation Accuracy:\", correct / total)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6TCnqjPlPUd",
        "outputId": "5e015687-05b2-4403-ed58-86592b434f36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# --------------------\n",
        "# Dataset Loader\n",
        "# --------------------\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, data_root, fixed_length=4096):\n",
        "        self.files = []\n",
        "        self.labels = []\n",
        "        self.data_root = data_root\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "        # Map folder name to integer labels\n",
        "        label_map = {'F':0, 'N':1, 'O':2, 'S':3, 'Z':4}\n",
        "\n",
        "        # Directory containing filtered EEG data\n",
        "        filtered_data_dir = os.path.join(data_root, 'EEGDATA_FILTERED')\n",
        "\n",
        "        for folder_name, label in label_map.items():\n",
        "            folder_path = os.path.join(filtered_data_dir, folder_name)\n",
        "            if os.path.isdir(folder_path):\n",
        "                for file in os.listdir(folder_path):\n",
        "                    if file.endswith(\".txt\") or file.endswith(\"_filtered.txt\"):\n",
        "                        self.files.append(os.path.join(folder_path, file))\n",
        "                        self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        signal = np.loadtxt(self.files[idx])\n",
        "\n",
        "        # Pad or truncate\n",
        "        if len(signal) < self.fixed_length:\n",
        "            signal = np.pad(signal, (0, self.fixed_length - len(signal)), 'constant')\n",
        "        else:\n",
        "            signal = signal[:self.fixed_length]\n",
        "\n",
        "        # Standardize signal\n",
        "        signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-8)\n",
        "\n",
        "        signal = torch.tensor(signal, dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return signal, label\n",
        "\n",
        "# --------------------\n",
        "# Improved CNN\n",
        "# --------------------\n",
        "class ImprovedEEGCNN(nn.Module):\n",
        "    def __init__(self, input_channels=1, num_classes=5):\n",
        "        super(ImprovedEEGCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = torch.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# --------------------\n",
        "# DataLoader\n",
        "# --------------------\n",
        "data_root = '/content/drive/MyDrive/EEG_DATA_REPO'\n",
        "dataset = EEGDataset(data_root)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "# --------------------\n",
        "# Training loop\n",
        "# --------------------\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = ImprovedEEGCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(100):  # train longer\n",
        "    model.train()\n",
        "    for x, y in train_loader:\n",
        "        x = x.unsqueeze(1).to(device)  # add channel dim\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1} done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkqiVP05BCcA",
        "outputId": "e78f6c8d-3c26-41c4-908c-a47e27edd038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 done\n",
            "Epoch 2 done\n",
            "Epoch 3 done\n",
            "Epoch 4 done\n",
            "Epoch 5 done\n",
            "Epoch 6 done\n",
            "Epoch 7 done\n",
            "Epoch 8 done\n",
            "Epoch 9 done\n",
            "Epoch 10 done\n",
            "Epoch 11 done\n",
            "Epoch 12 done\n",
            "Epoch 13 done\n",
            "Epoch 14 done\n",
            "Epoch 15 done\n",
            "Epoch 16 done\n",
            "Epoch 17 done\n",
            "Epoch 18 done\n",
            "Epoch 19 done\n",
            "Epoch 20 done\n",
            "Epoch 21 done\n",
            "Epoch 22 done\n",
            "Epoch 23 done\n",
            "Epoch 24 done\n",
            "Epoch 25 done\n",
            "Epoch 26 done\n",
            "Epoch 27 done\n",
            "Epoch 28 done\n",
            "Epoch 29 done\n",
            "Epoch 30 done\n",
            "Epoch 31 done\n",
            "Epoch 32 done\n",
            "Epoch 33 done\n",
            "Epoch 34 done\n",
            "Epoch 35 done\n",
            "Epoch 36 done\n",
            "Epoch 37 done\n",
            "Epoch 38 done\n",
            "Epoch 39 done\n",
            "Epoch 40 done\n",
            "Epoch 41 done\n",
            "Epoch 42 done\n",
            "Epoch 43 done\n",
            "Epoch 44 done\n",
            "Epoch 45 done\n",
            "Epoch 46 done\n",
            "Epoch 47 done\n",
            "Epoch 48 done\n",
            "Epoch 49 done\n",
            "Epoch 50 done\n",
            "Epoch 51 done\n",
            "Epoch 52 done\n",
            "Epoch 53 done\n",
            "Epoch 54 done\n",
            "Epoch 55 done\n",
            "Epoch 56 done\n",
            "Epoch 57 done\n",
            "Epoch 58 done\n",
            "Epoch 59 done\n",
            "Epoch 60 done\n",
            "Epoch 61 done\n",
            "Epoch 62 done\n",
            "Epoch 63 done\n",
            "Epoch 64 done\n",
            "Epoch 65 done\n",
            "Epoch 66 done\n",
            "Epoch 67 done\n",
            "Epoch 68 done\n",
            "Epoch 69 done\n",
            "Epoch 70 done\n",
            "Epoch 71 done\n",
            "Epoch 72 done\n",
            "Epoch 73 done\n",
            "Epoch 74 done\n",
            "Epoch 75 done\n",
            "Epoch 76 done\n",
            "Epoch 77 done\n",
            "Epoch 78 done\n",
            "Epoch 79 done\n",
            "Epoch 80 done\n",
            "Epoch 81 done\n",
            "Epoch 82 done\n",
            "Epoch 83 done\n",
            "Epoch 84 done\n",
            "Epoch 85 done\n",
            "Epoch 86 done\n",
            "Epoch 87 done\n",
            "Epoch 88 done\n",
            "Epoch 89 done\n",
            "Epoch 90 done\n",
            "Epoch 91 done\n",
            "Epoch 92 done\n",
            "Epoch 93 done\n",
            "Epoch 94 done\n",
            "Epoch 95 done\n",
            "Epoch 96 done\n",
            "Epoch 97 done\n",
            "Epoch 98 done\n",
            "Epoch 99 done\n",
            "Epoch 100 done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "        x = x.unsqueeze(1).to(device)\n",
        "        y = y.to(device)\n",
        "        out = model(x)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "print(\"Validation Accuracy:\", correct / total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdgQcwbdEs8j",
        "outputId": "43557193-75aa-4b48-e9d1-57b8ecca9e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM Begins here\n"
      ],
      "metadata": {
        "id": "tOpnUZ2rRIh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1: Load .txt files\n",
        "\n",
        "Each .txt contains a 1D signal (say, 4096 samples). We’ll load them, normalize, and stack into tensors."
      ],
      "metadata": {
        "id": "erCSir_2T3Ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# --- Config ---\n",
        "DATA_ROOT = \"/content/drive/MyDrive/EEG_DATA_REPO/EEGDATA_FILTERED\"\n",
        "LABELS = {'F':0, 'N':1, 'O':2, 'S':3, 'Z':4}   # adjust if needed\n",
        "SEQ_LEN = 4096   # number of timesteps per signal\n",
        "\n",
        "# --- Dataset ---\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, data_root, seq_len=4096):\n",
        "        self.samples = []\n",
        "        self.labels = []\n",
        "        for folder, label in LABELS.items():\n",
        "            folder_path = os.path.join(data_root, folder)\n",
        "            for fname in os.listdir(folder_path):\n",
        "                if fname.endswith(\".txt\"):\n",
        "                    sig = np.loadtxt(os.path.join(folder_path, fname))\n",
        "                    sig = sig[:seq_len]   # trim/pad\n",
        "                    if len(sig) < seq_len:\n",
        "                        sig = np.pad(sig, (0, seq_len - len(sig)))\n",
        "                    sig = (sig - np.mean(sig)) / (np.std(sig) + 1e-6)  # normalize\n",
        "                    self.samples.append(sig.astype(np.float32))\n",
        "                    self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.samples[idx]).unsqueeze(0)  # shape (1, seq_len)\n",
        "        y = torch.tensor(self.labels[idx]).long()\n",
        "        return x, y\n",
        "\n",
        "# build dataset\n",
        "dataset = EEGDataset(DATA_ROOT, seq_len=SEQ_LEN)\n",
        "\n",
        "# train/test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_idx, val_idx = train_test_split(np.arange(len(dataset)), test_size=0.2, stratify=dataset.labels, random_state=42)\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "train_loader = DataLoader(Subset(dataset, train_idx), batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(Subset(dataset, val_idx), batch_size=32)\n"
      ],
      "metadata": {
        "id": "DyZQDqE9Te5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2: Define LSTM Model"
      ],
      "metadata": {
        "id": "aN8vwzH2T8ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class EEG_LSTM(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, num_classes=5):\n",
        "        super(EEG_LSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
        "                            batch_first=True, dropout=0.3, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_size*2, num_classes)  # *2 for bidirectional\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, 1, seq_len) → (batch, seq_len, 1)\n",
        "        x = x.transpose(1,2)  # (batch, seq_len, 1)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])  # last time step\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "EDjrdcyIT8Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3: Training Loop"
      ],
      "metadata": {
        "id": "FWJr04GQUDks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EEG_LSTM().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
        "\n",
        "EPOCHS = 30\n",
        "for epoch in range(EPOCHS):\n",
        "    # --- Train ---\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)  # stabilize training\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (outputs.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation ---\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            val_correct += (outputs.argmax(1) == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss={total_loss:.3f}, Train Acc={train_acc:.3f}, Val Acc={val_acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TmY35R9UFKL",
        "outputId": "4d55eb5b-4611-419e-aef7-af7b1a25263a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss=20.940, Train Acc=0.200, Val Acc=0.210\n",
            "Epoch 2/30, Loss=20.885, Train Acc=0.215, Val Acc=0.240\n",
            "Epoch 3/30, Loss=20.833, Train Acc=0.240, Val Acc=0.200\n",
            "Epoch 4/30, Loss=20.669, Train Acc=0.258, Val Acc=0.200\n",
            "Epoch 5/30, Loss=20.424, Train Acc=0.255, Val Acc=0.260\n",
            "Epoch 6/30, Loss=20.118, Train Acc=0.275, Val Acc=0.260\n",
            "Epoch 7/30, Loss=19.931, Train Acc=0.312, Val Acc=0.260\n",
            "Epoch 8/30, Loss=19.991, Train Acc=0.285, Val Acc=0.230\n",
            "Epoch 9/30, Loss=19.768, Train Acc=0.305, Val Acc=0.220\n",
            "Epoch 10/30, Loss=19.614, Train Acc=0.315, Val Acc=0.270\n",
            "Epoch 11/30, Loss=19.507, Train Acc=0.315, Val Acc=0.260\n",
            "Epoch 12/30, Loss=19.559, Train Acc=0.312, Val Acc=0.250\n",
            "Epoch 13/30, Loss=19.464, Train Acc=0.333, Val Acc=0.250\n",
            "Epoch 14/30, Loss=19.477, Train Acc=0.305, Val Acc=0.230\n",
            "Epoch 15/30, Loss=19.233, Train Acc=0.325, Val Acc=0.260\n",
            "Epoch 16/30, Loss=19.213, Train Acc=0.333, Val Acc=0.290\n",
            "Epoch 17/30, Loss=19.154, Train Acc=0.335, Val Acc=0.310\n",
            "Epoch 18/30, Loss=19.000, Train Acc=0.347, Val Acc=0.360\n",
            "Epoch 19/30, Loss=18.829, Train Acc=0.365, Val Acc=0.310\n",
            "Epoch 20/30, Loss=17.817, Train Acc=0.425, Val Acc=0.430\n",
            "Epoch 21/30, Loss=16.992, Train Acc=0.455, Val Acc=0.400\n",
            "Epoch 22/30, Loss=17.430, Train Acc=0.435, Val Acc=0.410\n",
            "Epoch 23/30, Loss=16.360, Train Acc=0.450, Val Acc=0.420\n",
            "Epoch 24/30, Loss=16.600, Train Acc=0.458, Val Acc=0.420\n",
            "Epoch 25/30, Loss=15.931, Train Acc=0.445, Val Acc=0.430\n",
            "Epoch 26/30, Loss=15.996, Train Acc=0.450, Val Acc=0.320\n",
            "Epoch 27/30, Loss=16.775, Train Acc=0.443, Val Acc=0.400\n",
            "Epoch 28/30, Loss=15.348, Train Acc=0.485, Val Acc=0.410\n",
            "Epoch 29/30, Loss=15.333, Train Acc=0.492, Val Acc=0.410\n",
            "Epoch 30/30, Loss=15.919, Train Acc=0.485, Val Acc=0.350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Improved LSTM"
      ],
      "metadata": {
        "id": "bc3bz--Mi-xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# --- Config ---\n",
        "DATA_ROOT = \"/content/drive/MyDrive/EEG_DATA_REPO/EEGDATA_FILTERED\"\n",
        "LABELS = {'F':0, 'N':1, 'O':2, 'S':3, 'Z':4}   # adjust if needed\n",
        "SEQ_LEN = 4096   # number of timesteps per signal\n"
      ],
      "metadata": {
        "id": "txjoadmvi-GO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "id": "31iAkz1pjUky"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, data_root, seq_len=1024):  # shorter by default\n",
        "        self.samples = []\n",
        "        self.labels = []\n",
        "        for folder, label in LABELS.items():\n",
        "            folder_path = os.path.join(data_root, folder)\n",
        "            for fname in os.listdir(folder_path):\n",
        "                if fname.endswith(\".txt\"):\n",
        "                    sig = np.loadtxt(os.path.join(folder_path, fname))\n",
        "                    # --- downsample if signal is long ---\n",
        "                    factor = len(sig) // seq_len\n",
        "                    if factor > 1:\n",
        "                        sig = sig[::factor]  # pick every Nth sample\n",
        "                    # --- trim/pad to exact length ---\n",
        "                    sig = sig[:seq_len]\n",
        "                    if len(sig) < seq_len:\n",
        "                        sig = np.pad(sig, (0, seq_len - len(sig)))\n",
        "                    # --- normalize ---\n",
        "                    sig = (sig - np.mean(sig)) / (np.std(sig) + 1e-6)\n",
        "                    self.samples.append(sig.astype(np.float32))\n",
        "                    self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.samples[idx]).unsqueeze(0)  # (1, seq_len)\n",
        "        y = torch.tensor(self.labels[idx]).long()\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "ZwysAMpsVvex"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EEG_LSTM(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2, num_classes=5):\n",
        "        super(EEG_LSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
        "                            batch_first=True, dropout=0.5, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)            # (batch, seq_len, 1)\n",
        "        out, _ = self.lstm(x)            # (batch, seq_len, hidden*2)\n",
        "        out = torch.mean(out, dim=1)     # mean pooling over time\n",
        "        out = self.fc(out)               # classification\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "GVSmmyazi2tP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# build model\n",
        "model = EEG_LSTM().to(device)\n",
        "\n",
        "# loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer + scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', factor=0.5, patience=5\n",
        ")\n"
      ],
      "metadata": {
        "id": "XMVklPIfjmC5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Build model\n",
        "model = EEG_LSTM().to(device)\n",
        "\n",
        "# 2. Define loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 3. Define optimizer & scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', factor=0.5, patience=5\n",
        ")\n"
      ],
      "metadata": {
        "id": "lkyKf9Q3jgNm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',\n",
        "                                                 factor=0.5, patience=5)\n"
      ],
      "metadata": {
        "id": "OA7hma3Ei4hh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Build dataset ---\n",
        "dataset = EEGDataset(DATA_ROOT, seq_len=SEQ_LEN)\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(dataset)), test_size=0.2,\n",
        "    stratify=dataset.labels, random_state=42\n",
        ")\n",
        "train_loader = DataLoader(Subset(dataset, train_idx), batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(Subset(dataset, val_idx), batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "7Z79h_W3kJUR",
        "outputId": "c5e9f6b0-4929-40ce-c3c2-1ae2688969b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_test_split' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-671944968.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- Build dataset ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEEGDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEQ_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train_idx, val_idx = train_test_split(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30\n",
        "log = []\n",
        "\n",
        "# --- Training loop ---\n",
        "for epoch in range(EPOCHS):\n",
        "    # Train\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (outputs.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            val_correct += (outputs.argmax(1) == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    log.append([epoch+1, total_loss/len(train_loader), train_acc, val_acc])\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss={total_loss:.3f}, Train Acc={train_acc:.3f}, Val Acc={val_acc:.3f}\")\n",
        "\n",
        "# --- Log results table ---\n",
        "df_log = pd.DataFrame(log, columns=[\"Epoch\", \"Train Loss\", \"Train Acc\", \"Val Acc\"])\n",
        "print(df_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "T6uuE0-zj9-A",
        "outputId": "bb196259-77ed-4bf7-95a8-2a641dbce433"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3972562392.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# --- Config ---\n",
        "DATA_ROOT = \"/content/drive/MyDrive/EEG_DATA_REPO/EEGDATA_FILTERED\"\n",
        "LABELS = {'F':0, 'N':1, 'O':2, 'S':3, 'Z':4}\n",
        "SEQ_LEN = 1024  # shorter sequence (downsampled)\n",
        "\n",
        "# --- Dataset ---\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, data_root, seq_len=1024):\n",
        "        self.samples, self.labels = [], []\n",
        "        for folder, label in LABELS.items():\n",
        "            folder_path = os.path.join(data_root, folder)\n",
        "            for fname in os.listdir(folder_path):\n",
        "                if fname.endswith(\".txt\"):\n",
        "                    sig = np.loadtxt(os.path.join(folder_path, fname))\n",
        "                    # downsample\n",
        "                    factor = len(sig) // seq_len\n",
        "                    if factor > 1:\n",
        "                        sig = sig[::factor]\n",
        "                    # trim/pad\n",
        "                    sig = sig[:seq_len]\n",
        "                    if len(sig) < seq_len:\n",
        "                        sig = np.pad(sig, (0, seq_len - len(sig)))\n",
        "                    # normalize\n",
        "                    sig = (sig - np.mean(sig)) / (np.std(sig) + 1e-6)\n",
        "                    self.samples.append(sig.astype(np.float32))\n",
        "                    self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.samples[idx]).unsqueeze(0)  # (1, seq_len)\n",
        "        y = torch.tensor(self.labels[idx]).long()\n",
        "        return x, y\n",
        "\n",
        "# --- Build dataset ---\n",
        "dataset = EEGDataset(DATA_ROOT, seq_len=SEQ_LEN)\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(dataset)), test_size=0.2,\n",
        "    stratify=dataset.labels, random_state=42\n",
        ")\n",
        "train_loader = DataLoader(Subset(dataset, train_idx), batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(Subset(dataset, val_idx), batch_size=32)\n",
        "\n",
        "# --- Model ---\n",
        "class EEG_LSTM(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2, num_classes=5):\n",
        "        super(EEG_LSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
        "                            batch_first=True, dropout=0.5, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)            # (batch, seq_len, 1)\n",
        "        out, _ = self.lstm(x)            # (batch, seq_len, hidden*2)\n",
        "        out = torch.mean(out, dim=1)     # mean pooling\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# --- Training setup ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EEG_LSTM().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',\n",
        "                                                 factor=0.5, patience=5)\n",
        "\n",
        "EPOCHS = 30\n",
        "log = []\n",
        "\n",
        "# --- Training loop ---\n",
        "for epoch in range(EPOCHS):\n",
        "    # Train\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (outputs.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            val_correct += (outputs.argmax(1) == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    log.append([epoch+1, total_loss/len(train_loader), train_acc, val_acc])\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss={total_loss:.3f}, Train Acc={train_acc:.3f}, Val Acc={val_acc:.3f}\")\n",
        "\n",
        "# --- Log results table ---\n",
        "df_log = pd.DataFrame(log, columns=[\"Epoch\", \"Train Loss\", \"Train Acc\", \"Val Acc\"])\n",
        "print(df_log)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQyYq34CkQSo",
        "outputId": "4f068277-a58f-4c82-d0e1-4e266943b5d6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss=20.939, Train Acc=0.200, Val Acc=0.200\n",
            "Epoch 2/30, Loss=20.928, Train Acc=0.200, Val Acc=0.200\n",
            "Epoch 3/30, Loss=20.925, Train Acc=0.200, Val Acc=0.200\n",
            "Epoch 4/30, Loss=20.927, Train Acc=0.200, Val Acc=0.200\n",
            "Epoch 5/30, Loss=20.923, Train Acc=0.200, Val Acc=0.200\n",
            "Epoch 6/30, Loss=20.924, Train Acc=0.200, Val Acc=0.200\n",
            "Epoch 7/30, Loss=20.923, Train Acc=0.195, Val Acc=0.230\n",
            "Epoch 8/30, Loss=20.923, Train Acc=0.200, Val Acc=0.200\n",
            "Epoch 9/30, Loss=20.917, Train Acc=0.200, Val Acc=0.200\n",
            "Epoch 10/30, Loss=20.916, Train Acc=0.215, Val Acc=0.270\n",
            "Epoch 11/30, Loss=20.914, Train Acc=0.230, Val Acc=0.200\n",
            "Epoch 12/30, Loss=20.910, Train Acc=0.223, Val Acc=0.200\n",
            "Epoch 13/30, Loss=20.896, Train Acc=0.223, Val Acc=0.200\n",
            "Epoch 14/30, Loss=20.886, Train Acc=0.200, Val Acc=0.200\n",
            "Epoch 15/30, Loss=20.857, Train Acc=0.200, Val Acc=0.200\n",
            "Epoch 16/30, Loss=20.802, Train Acc=0.217, Val Acc=0.200\n",
            "Epoch 17/30, Loss=20.697, Train Acc=0.212, Val Acc=0.160\n",
            "Epoch 18/30, Loss=20.464, Train Acc=0.220, Val Acc=0.230\n",
            "Epoch 19/30, Loss=21.410, Train Acc=0.242, Val Acc=0.280\n",
            "Epoch 20/30, Loss=20.176, Train Acc=0.343, Val Acc=0.390\n",
            "Epoch 21/30, Loss=19.914, Train Acc=0.343, Val Acc=0.470\n",
            "Epoch 22/30, Loss=19.645, Train Acc=0.338, Val Acc=0.370\n",
            "Epoch 23/30, Loss=19.221, Train Acc=0.340, Val Acc=0.380\n",
            "Epoch 24/30, Loss=18.747, Train Acc=0.362, Val Acc=0.400\n",
            "Epoch 25/30, Loss=18.177, Train Acc=0.385, Val Acc=0.440\n",
            "Epoch 26/30, Loss=17.418, Train Acc=0.375, Val Acc=0.420\n",
            "Epoch 27/30, Loss=16.911, Train Acc=0.395, Val Acc=0.490\n",
            "Epoch 28/30, Loss=16.906, Train Acc=0.350, Val Acc=0.430\n",
            "Epoch 29/30, Loss=16.876, Train Acc=0.393, Val Acc=0.430\n",
            "Epoch 30/30, Loss=17.031, Train Acc=0.380, Val Acc=0.410\n",
            "    Epoch  Train Loss  Train Acc  Val Acc\n",
            "0       1    1.610711     0.2000     0.20\n",
            "1       2    1.609839     0.2000     0.20\n",
            "2       3    1.609603     0.2000     0.20\n",
            "3       4    1.609773     0.2000     0.20\n",
            "4       5    1.609452     0.2000     0.20\n",
            "5       6    1.609561     0.2000     0.20\n",
            "6       7    1.609439     0.1950     0.23\n",
            "7       8    1.609476     0.2000     0.20\n",
            "8       9    1.609014     0.2000     0.20\n",
            "9      10    1.608928     0.2150     0.27\n",
            "10     11    1.608764     0.2300     0.20\n",
            "11     12    1.608453     0.2225     0.20\n",
            "12     13    1.607397     0.2225     0.20\n",
            "13     14    1.606580     0.2000     0.20\n",
            "14     15    1.604417     0.2000     0.20\n",
            "15     16    1.600181     0.2175     0.20\n",
            "16     17    1.592047     0.2125     0.16\n",
            "17     18    1.574126     0.2200     0.23\n",
            "18     19    1.646923     0.2425     0.28\n",
            "19     20    1.552026     0.3425     0.39\n",
            "20     21    1.531845     0.3425     0.47\n",
            "21     22    1.511126     0.3375     0.37\n",
            "22     23    1.478505     0.3400     0.38\n",
            "23     24    1.442072     0.3625     0.40\n",
            "24     25    1.398219     0.3850     0.44\n",
            "25     26    1.339878     0.3750     0.42\n",
            "26     27    1.300811     0.3950     0.49\n",
            "27     28    1.300458     0.3500     0.43\n",
            "28     29    1.298126     0.3925     0.43\n",
            "29     30    1.310051     0.3800     0.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGBoost dataset"
      ],
      "metadata": {
        "id": "ueWm3tGbCVt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= Imports =========\n",
        "import os, glob, json, joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.signal import welch\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ========= Config =========\n",
        "DATA_ROOT = \"/content/drive/MyDrive/EEG_DATA_REPO/EEGDATA_FILTERED\"\n",
        "LABELS = {'F':0, 'N':1, 'O':2, 'S':3, 'Z':4}\n",
        "ASSUMED_FS = 173.61    # Bonn sampling rate; tweak if yours differs\n",
        "NPERSEG = 256          # Welch window\n",
        "CACHE_FEATS = \"/content/eeg_features.parquet\"  # cache to skip recompute later\n",
        "MODEL_OUT   = \"/content/xgb_bonn_model.joblib\"\n",
        "FEATS_OUT   = \"/content/eeg_features.parquet\"\n",
        "REPORT_OUT  = \"/content/xgb_report.json\"\n",
        "\n",
        "# ========= Feature Engineering =========\n",
        "def bandpower_welch(sig, fs, bands, nperseg=256):\n",
        "    f, Pxx = welch(sig, fs=fs, nperseg=min(nperseg, len(sig)))\n",
        "    out = {}\n",
        "    total_power = np.trapz(Pxx, f) + 1e-12\n",
        "    for name, (lo, hi) in bands.items():\n",
        "        idx = (f >= lo) & (f <= hi)\n",
        "        bp = np.trapz(Pxx[idx], f[idx])\n",
        "        out[f\"pow_{name}\"] = bp\n",
        "        out[f\"relpow_{name}\"] = bp / total_power\n",
        "    return out\n",
        "\n",
        "def hjorth_params(sig):\n",
        "    diff1 = np.diff(sig)\n",
        "    diff2 = np.diff(diff1)\n",
        "    var0 = np.var(sig) + 1e-12\n",
        "    var1 = np.var(diff1) + 1e-12\n",
        "    var2 = np.var(diff2) + 1e-12\n",
        "    mobility = np.sqrt(var1/var0)\n",
        "    complexity = np.sqrt(var2/var1) / mobility\n",
        "    return {\n",
        "        \"hj_activity\": var0,\n",
        "        \"hj_mobility\": mobility,\n",
        "        \"hj_complexity\": complexity,\n",
        "    }\n",
        "\n",
        "def extract_features(signal, fs=ASSUMED_FS):\n",
        "    sig = np.asarray(signal, dtype=np.float64)\n",
        "    # robust z-score (optional)\n",
        "    sig = (sig - np.mean(sig)) / (np.std(sig) + 1e-12)\n",
        "\n",
        "    feats = {}\n",
        "    # Time domain\n",
        "    feats[\"mean\"] = np.mean(sig)\n",
        "    feats[\"std\"] = np.std(sig)\n",
        "    feats[\"skew\"] = skew(sig)\n",
        "    feats[\"kurtosis\"] = kurtosis(sig)\n",
        "    feats[\"ptp\"] = np.ptp(sig)  # peak-to-peak\n",
        "    feats[\"rms\"] = np.sqrt(np.mean(sig**2))\n",
        "\n",
        "    # Hjorth\n",
        "    feats.update(hjorth_params(sig))\n",
        "\n",
        "    # Frequency domain\n",
        "    bands = {\n",
        "        \"delta\": (0.5, 4),\n",
        "        \"theta\": (4, 8),\n",
        "        \"alpha\": (8, 13),\n",
        "        \"beta\":  (13, 30),\n",
        "        \"gamma\": (30, 45),\n",
        "    }\n",
        "    feats.update(bandpower_welch(sig, fs, bands, nperseg=NPERSEG))\n",
        "\n",
        "    # Ratios that often help EEG classification\n",
        "    feats[\"alpha_beta_ratio\"] = feats[\"pow_alpha\"] / (feats[\"pow_beta\"] + 1e-12)\n",
        "    feats[\"theta_alpha_ratio\"] = feats[\"pow_theta\"] / (feats[\"pow_alpha\"] + 1e-12)\n",
        "    feats[\"delta_theta_ratio\"] = feats[\"pow_delta\"] / (feats[\"pow_theta\"] + 1e-12)\n",
        "\n",
        "    return feats\n",
        "\n",
        "# ========= Data Loader (reads your folders) =========\n",
        "def load_signals_from_root(data_root, labels_map):\n",
        "    X, y, meta = [], [], []\n",
        "    for folder, lbl in labels_map.items():\n",
        "        folder_path = os.path.join(data_root, folder)\n",
        "        if not os.path.isdir(folder_path):\n",
        "            print(f\"[WARN] Missing folder: {folder_path}\")\n",
        "            continue\n",
        "        for fp in glob.glob(os.path.join(folder_path, \"*.txt\")):\n",
        "            try:\n",
        "                sig = np.loadtxt(fp)\n",
        "                # sanitize: replace NaN/inf\n",
        "                sig = np.nan_to_num(sig, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                X.append(sig.astype(np.float32))\n",
        "                y.append(lbl)\n",
        "                meta.append({\"path\": fp, \"folder\": folder})\n",
        "            except Exception as e:\n",
        "                print(f\"[SKIP] {fp} due to {e}\")\n",
        "    return X, np.array(y, dtype=int), pd.DataFrame(meta)\n",
        "\n",
        "# ========= Build Feature Table =========\n",
        "if os.path.exists(CACHE_FEATS):\n",
        "    df = pd.read_parquet(CACHE_FEATS)\n",
        "    y_all = df.pop(\"label\").values.astype(int)\n",
        "    meta = df.pop(\"path\") if \"path\" in df.columns else None\n",
        "    X_all = df\n",
        "    print(f\"[CACHE] Loaded features: {X_all.shape}\")\n",
        "else:\n",
        "    X_raw, y_all, meta = load_signals_from_root(DATA_ROOT, LABELS)\n",
        "    feature_rows = []\n",
        "    for i, sig in enumerate(X_raw):\n",
        "        feats = extract_features(sig, fs=ASSUMED_FS)\n",
        "        feats[\"label\"] = int(y_all[i])\n",
        "        feats[\"path\"]  = meta.loc[i, \"path\"]\n",
        "        feature_rows.append(feats)\n",
        "    df = pd.DataFrame(feature_rows)\n",
        "    df.to_parquet(CACHE_FEATS, index=False)\n",
        "    print(f\"[SAVE] Features cached to {CACHE_FEATS}\")\n",
        "    y_all = df.pop(\"label\").values.astype(int)\n",
        "    meta = df.pop(\"path\") if \"path\" in df.columns else None\n",
        "    X_all = df\n",
        "\n",
        "print(\"Feature matrix:\", X_all.shape)\n",
        "\n",
        "# ========= Quick single split score (fast sanity check) =========\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X_all, y_all, test_size=0.2, stratify=y_all, random_state=42\n",
        ")\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"xgb\", xgb.XGBClassifier(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=6,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.9,\n",
        "        reg_lambda=1.0,\n",
        "        reg_alpha=0.0,\n",
        "        objective=\"multi:softprob\",\n",
        "        eval_metric=\"mlogloss\",\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "    ))\n",
        "])\n",
        "\n",
        "pipe.fit(X_tr, y_tr)\n",
        "pred = pipe.predict(X_te)\n",
        "acc = accuracy_score(y_te, pred)\n",
        "print(f\"[Holdout] Accuracy: {acc:.3f}\")\n",
        "print(\"\\n[Holdout] Classification Report:\\n\", classification_report(y_te, pred, digits=3))\n",
        "\n",
        "# ========= 5-Fold Stratified CV (robust benchmark) =========\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = []\n",
        "for fold, (tr, va) in enumerate(skf.split(X_all, y_all), 1):\n",
        "    pipe.fit(X_all.iloc[tr], y_all[tr])\n",
        "    p = pipe.predict(X_all.iloc[va])\n",
        "    a = accuracy_score(y_all[va], p)\n",
        "    cv_scores.append(a)\n",
        "    print(f\"[CV] Fold {fold}: {a:.3f}\")\n",
        "\n",
        "print(f\"[CV] Mean Accuracy: {np.mean(cv_scores):.3f} ± {np.std(cv_scores):.3f}\")\n",
        "\n",
        "# ========= Feature Importance (gain) =========\n",
        "# Fit once on full data to inspect importances\n",
        "pipe.fit(X_all, y_all)\n",
        "booster = pipe.named_steps[\"xgb\"]\n",
        "gain_importance = booster.get_booster().get_score(importance_type=\"gain\")\n",
        "\n",
        "# Map back to column names\n",
        "imp_df = pd.DataFrame([\n",
        "    {\"feature\": X_all.columns[int(k.replace('f',''))], \"gain\": v}\n",
        "    for k, v in gain_importance.items()\n",
        "]).sort_values(\"gain\", ascending=False)\n",
        "\n",
        "print(\"\\nTop features by gain:\\n\", imp_df.head(15))\n",
        "\n",
        "# Plot (optional)\n",
        "plt.figure(figsize=(7,5))\n",
        "topk = imp_df.head(15)\n",
        "plt.barh(topk[\"feature\"], topk[\"gain\"])\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"XGBoost Feature Importance (gain)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ========= Persist artifacts =========\n",
        "joblib.dump(pipe, MODEL_OUT)\n",
        "X_all.assign(label=y_all).to_parquet(FEATS_OUT, index=False)\n",
        "with open(REPORT_OUT, \"w\") as f:\n",
        "    json.dump({\n",
        "        \"holdout_acc\": float(acc),\n",
        "        \"cv_mean_acc\": float(np.mean(cv_scores)),\n",
        "        \"cv_std_acc\": float(np.std(cv_scores)),\n",
        "        \"top_features\": topk.to_dict(orient=\"records\")\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"\\n[Saved] Model -> {MODEL_OUT}\")\n",
        "print(f\"[Saved] Features -> {FEATS_OUT}\")\n",
        "print(f\"[Saved] Report -> {REPORT_OUT}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rV8bC_K2LtA5",
        "outputId": "892790b4-576f-4ac9-bf20-c2238e69bd71"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2046577961.py:31: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  total_power = np.trapz(Pxx, f) + 1e-12\n",
            "/tmp/ipython-input-2046577961.py:34: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  bp = np.trapz(Pxx[idx], f[idx])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVE] Features cached to /content/eeg_features.parquet\n",
            "Feature matrix: (500, 22)\n",
            "[Holdout] Accuracy: 0.900\n",
            "\n",
            "[Holdout] Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0      0.750     0.900     0.818        20\n",
            "           1      0.944     0.850     0.895        20\n",
            "           2      0.950     0.950     0.950        20\n",
            "           3      0.950     0.950     0.950        20\n",
            "           4      0.944     0.850     0.895        20\n",
            "\n",
            "    accuracy                          0.900       100\n",
            "   macro avg      0.908     0.900     0.902       100\n",
            "weighted avg      0.908     0.900     0.902       100\n",
            "\n",
            "[CV] Fold 1: 0.900\n",
            "[CV] Fold 2: 0.920\n",
            "[CV] Fold 3: 0.910\n",
            "[CV] Fold 4: 0.870\n",
            "[CV] Fold 5: 0.870\n",
            "[CV] Mean Accuracy: 0.894 ± 0.021\n",
            "\n",
            "Top features by gain:\n",
            "               feature      gain\n",
            "6         hj_activity  4.179671\n",
            "1                 std  3.885171\n",
            "7         hj_mobility  2.986378\n",
            "16        relpow_beta  2.849780\n",
            "5                 rms  2.568043\n",
            "13          pow_alpha  2.550464\n",
            "18       relpow_gamma  2.271942\n",
            "9           pow_delta  1.846067\n",
            "15           pow_beta  1.824259\n",
            "14       relpow_alpha  1.528944\n",
            "10       relpow_delta  1.071916\n",
            "8       hj_complexity  0.994218\n",
            "20  theta_alpha_ratio  0.968509\n",
            "17          pow_gamma  0.652007\n",
            "0                mean  0.587177\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Saved] Model -> /content/xgb_bonn_model.joblib\n",
            "[Saved] Features -> /content/eeg_features.parquet\n",
            "[Saved] Report -> /content/xgb_report.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# CNN + LSTM for Bonn EEG\n",
        "# =========================\n",
        "import os, glob, math, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ---------- Config ----------\n",
        "DATA_ROOT = \"/content/drive/MyDrive/EEG_DATA_REPO/EEGDATA_FILTERED\"\n",
        "LABELS = {'F':0, 'N':1, 'O':2, 'S':3, 'Z':4}   # maps folder -> class id\n",
        "SEQ_LEN = 4096\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 40\n",
        "LR = 1e-3\n",
        "PATIENCE = 8                 # early stopping patience (epochs)\n",
        "MODEL_OUT = \"/content/cnn_lstm_bonn.pt\"\n",
        "SEED = 42\n",
        "\n",
        "# ---------- Repro ----------\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# ---------- Dataset ----------\n",
        "class BonnEEGDataset(Dataset):\n",
        "    def __init__(self, data_root, labels_map, seq_len=4096, augment=False):\n",
        "        self.paths, self.y = [], []\n",
        "        for folder, y in labels_map.items():\n",
        "            fdir = os.path.join(data_root, folder)\n",
        "            files = sorted(glob.glob(os.path.join(fdir, \"*.txt\")))\n",
        "            for fp in files:\n",
        "                self.paths.append(fp)\n",
        "                self.y.append(y)\n",
        "        self.seq_len = seq_len\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self): return len(self.paths)\n",
        "\n",
        "    def _zscore(self, x):\n",
        "        return (x - x.mean()) / (x.std() + 1e-6)\n",
        "\n",
        "    def _random_time_shift(self, x, max_shift=64):\n",
        "        if max_shift <= 0: return x\n",
        "        s = np.random.randint(-max_shift, max_shift+1)\n",
        "        return np.roll(x, s)\n",
        "\n",
        "    def _random_dropout(self, x, max_frac=0.03):\n",
        "        # zero-out a short random chunk\n",
        "        if max_frac <= 0: return x\n",
        "        L = len(x)\n",
        "        w = np.random.randint(1, int(L*max_frac)+1)\n",
        "        i = np.random.randint(0, L-w+1)\n",
        "        x = x.copy()\n",
        "        x[i:i+w] = 0.0\n",
        "        return x\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sig = np.loadtxt(self.paths[idx]).astype(np.float32)\n",
        "        # trim/pad\n",
        "        sig = sig[:self.seq_len]\n",
        "        if len(sig) < self.seq_len:\n",
        "            sig = np.pad(sig, (0, self.seq_len - len(sig)))\n",
        "        # per-sample z-score\n",
        "        sig = self._zscore(sig)\n",
        "\n",
        "        if self.augment:\n",
        "            # light, label-preserving augmentations\n",
        "            if np.random.rand() < 0.5:\n",
        "                sig = self._random_time_shift(sig, max_shift=64)\n",
        "            if np.random.rand() < 0.3:\n",
        "                sig = self._random_dropout(sig, max_frac=0.03)\n",
        "\n",
        "        # shape (channels=1, seq_len)\n",
        "        x = torch.tensor(sig).unsqueeze(0)   # (1, 4096)\n",
        "        y = torch.tensor(self.y[idx]).long()\n",
        "        return x, y\n",
        "\n",
        "# ---------- Data split ----------\n",
        "full_ds = BonnEEGDataset(DATA_ROOT, LABELS, seq_len=SEQ_LEN, augment=False)\n",
        "\n",
        "# Stratified indices\n",
        "indices = np.arange(len(full_ds))\n",
        "y_all = np.array(full_ds.y)\n",
        "train_idx, val_idx = train_test_split(\n",
        "    indices, test_size=0.2, random_state=SEED, stratify=y_all\n",
        ")\n",
        "\n",
        "# Augment only on train\n",
        "train_ds = BonnEEGDataset(DATA_ROOT, LABELS, seq_len=SEQ_LEN, augment=True)\n",
        "val_ds   = BonnEEGDataset(DATA_ROOT, LABELS, seq_len=SEQ_LEN, augment=False)\n",
        "\n",
        "train_loader = DataLoader(Subset(train_ds, train_idx), batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(Subset(val_ds,   val_idx),   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# ---------- Model ----------\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, k=7, p=None):\n",
        "        super().__init__()\n",
        "        pad = (k//2) if p is None else p\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=pad),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(out_ch, out_ch, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool1d(kernel_size=2)  # /2\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self, n_classes, n_channels=1, seq_len=4096, lstm_hidden=128, lstm_layers=1):\n",
        "        super().__init__()\n",
        "        # CNN front-end (extract local motifs)\n",
        "        self.c1 = ConvBlock(n_channels, 32, k=9)   # wide kernel first\n",
        "        self.c2 = ConvBlock(32, 64, k=7)\n",
        "        self.c3 = ConvBlock(64, 128, k=5)\n",
        "        # after 3 pools, seq_len reduces by /8\n",
        "        self.reduced_len = seq_len // 8\n",
        "        # LSTM (temporal integration over feature sequence)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=128, hidden_size=lstm_hidden, num_layers=lstm_layers,\n",
        "            batch_first=True, bidirectional=True, dropout=0.0 if lstm_layers==1 else 0.2\n",
        "        )\n",
        "        # Attention-style pooling (mean + max)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(lstm_hidden*2*2, 128),  # concat(mean,max)\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(128, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, 1, 4096)\n",
        "        x = self.c1(x)      # (B, 32, 2048)\n",
        "        x = self.c2(x)      # (B, 64, 1024)\n",
        "        x = self.c3(x)      # (B,128, 512)\n",
        "        x = x.permute(0, 2, 1)  # (B, T=512, F=128)\n",
        "        out, _ = self.lstm(x)   # (B, T, 2*hidden)\n",
        "        # temporal pooling\n",
        "        mean_pool = out.mean(dim=1)\n",
        "        max_pool, _ = out.max(dim=1)\n",
        "        feats = torch.cat([mean_pool, max_pool], dim=1)  # (B, 2*hidden*2)\n",
        "        logits = self.head(feats)\n",
        "        return logits\n",
        "\n",
        "# ---------- Train/Eval ----------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNN_LSTM(n_classes=len(LABELS), n_channels=1, seq_len=SEQ_LEN, lstm_hidden=128, lstm_layers=1).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "best_val = 0.0\n",
        "epochs_no_improve = 0\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    all_y, all_p = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "            with autocast(enabled=torch.cuda.is_available()):\n",
        "                logits = model(xb)\n",
        "            preds = logits.argmax(1)\n",
        "            all_y.append(yb.cpu().numpy())\n",
        "            all_p.append(preds.cpu().numpy())\n",
        "    y_true = np.concatenate(all_y); y_pred = np.concatenate(all_p)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return acc, y_true, y_pred\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with autocast(enabled=torch.cuda.is_available()):\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "        scaler.scale(loss).backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # val\n",
        "    val_acc, y_true, y_pred = evaluate(model, val_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch:02d}/{EPOCHS} | Train Loss={running_loss:.3f} | Val Acc={val_acc:.3f}\")\n",
        "    if val_acc > best_val + 1e-4:\n",
        "        best_val = val_acc\n",
        "        epochs_no_improve = 0\n",
        "        torch.save({\"model_state\": model.state_dict(),\n",
        "                    \"val_acc\": best_val,\n",
        "                    \"epoch\": epoch}, MODEL_OUT)\n",
        "        print(f\"  ✅ New best. Saved -> {MODEL_OUT}\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= PATIENCE:\n",
        "            print(\"  ⛔ Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# ---------- Final report (best checkpoint) ----------\n",
        "ckpt = torch.load(MODEL_OUT, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "val_acc, y_true, y_pred = evaluate(model, val_loader)\n",
        "print(f\"\\nBest Val Acc (reloaded): {val_acc:.3f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits=3))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUCpDT5Ad6gv",
        "outputId": "218acfb1-f43d-434b-901e-3564a7540013"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1448877933.py:160: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=torch.cuda.is_available())\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/40 | Train Loss=19.580 | Val Acc=0.200\n",
            "  ✅ New best. Saved -> /content/cnn_lstm_bonn.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02/40 | Train Loss=14.328 | Val Acc=0.620\n",
            "  ✅ New best. Saved -> /content/cnn_lstm_bonn.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03/40 | Train Loss=10.823 | Val Acc=0.410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04/40 | Train Loss=9.043 | Val Acc=0.690\n",
            "  ✅ New best. Saved -> /content/cnn_lstm_bonn.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05/40 | Train Loss=7.794 | Val Acc=0.760\n",
            "  ✅ New best. Saved -> /content/cnn_lstm_bonn.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06/40 | Train Loss=7.459 | Val Acc=0.780\n",
            "  ✅ New best. Saved -> /content/cnn_lstm_bonn.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07/40 | Train Loss=7.397 | Val Acc=0.770\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08/40 | Train Loss=6.185 | Val Acc=0.780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09/40 | Train Loss=6.036 | Val Acc=0.780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/40 | Train Loss=6.822 | Val Acc=0.820\n",
            "  ✅ New best. Saved -> /content/cnn_lstm_bonn.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/40 | Train Loss=6.052 | Val Acc=0.860\n",
            "  ✅ New best. Saved -> /content/cnn_lstm_bonn.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/40 | Train Loss=5.699 | Val Acc=0.890\n",
            "  ✅ New best. Saved -> /content/cnn_lstm_bonn.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/40 | Train Loss=5.419 | Val Acc=0.850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/40 | Train Loss=5.377 | Val Acc=0.810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/40 | Train Loss=6.263 | Val Acc=0.800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/40 | Train Loss=4.801 | Val Acc=0.850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/40 | Train Loss=4.312 | Val Acc=0.900\n",
            "  ✅ New best. Saved -> /content/cnn_lstm_bonn.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/40 | Train Loss=3.565 | Val Acc=0.800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/40 | Train Loss=3.880 | Val Acc=0.800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/40 | Train Loss=3.664 | Val Acc=0.870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/40 | Train Loss=3.070 | Val Acc=0.890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/40 | Train Loss=2.344 | Val Acc=0.790\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/40 | Train Loss=2.396 | Val Acc=0.870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/40 | Train Loss=2.080 | Val Acc=0.880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/40 | Train Loss=2.268 | Val Acc=0.880\n",
            "  ⛔ Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-1448877933.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Val Acc (reloaded): 0.900\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0      1.000     0.750     0.857        20\n",
            "           1      0.773     0.850     0.810        20\n",
            "           2      0.909     1.000     0.952        20\n",
            "           3      1.000     0.950     0.974        20\n",
            "           4      0.864     0.950     0.905        20\n",
            "\n",
            "    accuracy                          0.900       100\n",
            "   macro avg      0.909     0.900     0.900       100\n",
            "weighted avg      0.909     0.900     0.900       100\n",
            "\n",
            "Confusion Matrix:\n",
            " [[15  5  0  0  0]\n",
            " [ 0 17  0  0  3]\n",
            " [ 0  0 20  0  0]\n",
            " [ 0  0  1 19  0]\n",
            " [ 0  0  1  0 19]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN-LSTM-XGBoost"
      ],
      "metadata": {
        "id": "MhzG9QFCpYej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# CNN–LSTM → XGBoost Pipeline\n",
        "# Task: Seizure detection (binary)\n",
        "# Positive class: folder 'S'\n",
        "# ============================\n",
        "import os, glob, random, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,\n",
        "                             confusion_matrix, roc_auc_score, roc_curve, classification_report)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "# -------- Config --------\n",
        "DATA_ROOT = \"/content/drive/MyDrive/EEG_DATA_REPO/EEGDATA_FILTERED\"\n",
        "ALL_LABELS = {'F':0, 'N':1, 'O':2, 'S':3, 'Z':4}     # existing mapping\n",
        "SEIZURE_FOLDERS = {'S'}                               # positive class\n",
        "SEQ_LEN = 4096\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 25\n",
        "LR = 1e-3\n",
        "PATIENCE = 6\n",
        "SEED = 42\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_OUT = \"/content/cnn_lstm_seizure.pt\"\n",
        "\n",
        "# -------- Repro ----------\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# -------- Dataset --------\n",
        "class BonnEEGBinary(Dataset):\n",
        "    def __init__(self, data_root, seq_len=4096, augment=False):\n",
        "        self.seq_len = seq_len\n",
        "        self.augment = augment\n",
        "        self.paths, self.y = [], []\n",
        "\n",
        "        # scan folders\n",
        "        for folder in sorted(os.listdir(data_root)):\n",
        "            fpath = os.path.join(data_root, folder)\n",
        "            if not os.path.isdir(fpath): continue\n",
        "            files = sorted(glob.glob(os.path.join(fpath, \"*.txt\")))\n",
        "            for fp in files:\n",
        "                self.paths.append(fp)\n",
        "                # binary label: 1 if seizure folder 'S'\n",
        "                self.y.append(1 if os.path.basename(fpath) in SEIZURE_FOLDERS else 0)\n",
        "\n",
        "        self.y = np.array(self.y, dtype=np.int64)\n",
        "\n",
        "    def __len__(self): return len(self.paths)\n",
        "\n",
        "    def _zscore(self, x): return (x - x.mean()) / (x.std() + 1e-6)\n",
        "\n",
        "    def _time_shift(self, x, max_shift=64):\n",
        "        if max_shift <= 0: return x\n",
        "        s = np.random.randint(-max_shift, max_shift+1)\n",
        "        return np.roll(x, s)\n",
        "\n",
        "    def _drop_chunk(self, x, max_frac=0.03):\n",
        "        if max_frac <= 0: return x\n",
        "        L = len(x)\n",
        "        w = np.random.randint(1, max(2, int(L*max_frac)))\n",
        "        i = np.random.randint(0, L-w+1)\n",
        "        x = x.copy(); x[i:i+w] = 0.0\n",
        "        return x\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sig = np.loadtxt(self.paths[idx]).astype(np.float32)\n",
        "        sig = sig[:self.seq_len]\n",
        "        if len(sig) < self.seq_len:\n",
        "            sig = np.pad(sig, (0, self.seq_len - len(sig)))\n",
        "\n",
        "        sig = self._zscore(sig)\n",
        "\n",
        "        if self.augment:\n",
        "            if np.random.rand() < 0.5:\n",
        "                sig = self._time_shift(sig, 64)\n",
        "            if np.random.rand() < 0.3:\n",
        "                sig = self._drop_chunk(sig, 0.03)\n",
        "\n",
        "        x = torch.tensor(sig).unsqueeze(0)   # (1, 4096)\n",
        "        y = torch.tensor(self.y[idx]).long() # 0/1\n",
        "        return x, y\n",
        "\n",
        "# build dataset & stratified split (binary)\n",
        "full_ds = BonnEEGBinary(DATA_ROOT, seq_len=SEQ_LEN, augment=False)\n",
        "indices = np.arange(len(full_ds))\n",
        "y_all = full_ds.y\n",
        "train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=SEED, stratify=y_all)\n",
        "\n",
        "train_ds = BonnEEGBinary(DATA_ROOT, seq_len=SEQ_LEN, augment=True)\n",
        "val_ds   = BonnEEGBinary(DATA_ROOT, seq_len=SEQ_LEN, augment=False)\n",
        "train_loader = DataLoader(Subset(train_ds, train_idx), batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(Subset(val_ds,   val_idx),   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# -------- Model (CNN + LSTM) --------\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, k=7):\n",
        "        super().__init__()\n",
        "        pad = k // 2\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=pad),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(out_ch, out_ch, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool1d(kernel_size=2)  # /2\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class CNN_LSTM_FE(nn.Module):\n",
        "    \"\"\"\n",
        "    Feature extractor head returns a pooled sequence embedding.\n",
        "    We'll use that embedding for XGBoost.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_channels=1, seq_len=4096, lstm_hidden=128, lstm_layers=1, p_drop=0.4):\n",
        "        super().__init__()\n",
        "        self.c1 = ConvBlock(n_channels, 32, k=9)\n",
        "        self.c2 = ConvBlock(32, 64, k=7)\n",
        "        self.c3 = ConvBlock(64, 128, k=5)\n",
        "        self.reduced_len = seq_len // 8\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=128, hidden_size=lstm_hidden, num_layers=lstm_layers,\n",
        "            batch_first=True, bidirectional=True, dropout=0.0 if lstm_layers==1 else 0.2\n",
        "        )\n",
        "        self.dropout = nn.Dropout(p_drop)\n",
        "        # small binary head for pretraining\n",
        "        self.cls = nn.Linear(4*lstm_hidden, 2)  # (mean+max) of biLSTM -> 4*hidden\n",
        "\n",
        "    def forward(self, x, return_embedding=False):\n",
        "        # x: (B, 1, 4096)\n",
        "        x = self.c1(x)      # (B, 32, 2048)\n",
        "        x = self.c2(x)      # (B, 64, 1024)\n",
        "        x = self.c3(x)      # (B,128, 512)\n",
        "        x = x.permute(0, 2, 1)     # (B, T=512, F=128)\n",
        "        h, _ = self.lstm(x)        # (B, T, 2*hidden)\n",
        "        mean_pool = h.mean(dim=1)  # (B, 2*hidden)\n",
        "        max_pool, _ = h.max(dim=1) # (B, 2*hidden)\n",
        "        feats = torch.cat([mean_pool, max_pool], dim=1)  # (B, 4*hidden)\n",
        "        feats = self.dropout(feats)\n",
        "        logits = self.cls(feats)\n",
        "        if return_embedding:\n",
        "            return logits, feats\n",
        "        return logits\n",
        "\n",
        "# -------- Train FE on binary labels (for stable embeddings) --------\n",
        "def train_feature_extractor(train_loader, val_loader, epochs=EPOCHS, lr=LR, out_path=MODEL_OUT):\n",
        "    model = CNN_LSTM_FE(n_channels=1, seq_len=SEQ_LEN, lstm_hidden=128, lstm_layers=1, p_drop=0.4).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "    scaler = GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "    best_val, no_improve = 0.0, 0\n",
        "    def eval_bin(loader):\n",
        "        model.eval(); ys, ps = [], []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in loader:\n",
        "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "                with autocast(enabled=torch.cuda.is_available()):\n",
        "                    logits = model(xb)\n",
        "                    preds = logits.argmax(dim=1)\n",
        "                ys.append(yb.cpu().numpy()); ps.append(preds.cpu().numpy())\n",
        "        y_true = np.concatenate(ys); y_pred = np.concatenate(ps)\n",
        "        return accuracy_score(y_true, y_pred), y_true, y_pred\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train(); run_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with autocast(enabled=torch.cuda.is_available()):\n",
        "                logits = model(xb)\n",
        "                loss = criterion(logits, yb)\n",
        "            scaler.scale(loss).backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
        "            scaler.step(optimizer); scaler.update()\n",
        "            run_loss += loss.item()\n",
        "\n",
        "        val_acc, _, _ = eval_bin(val_loader)\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch {ep:02d}/{epochs} | TrainLoss={run_loss:.3f} | ValAcc={val_acc:.3f}\")\n",
        "\n",
        "        if val_acc > best_val + 1e-4:\n",
        "            best_val, no_improve = val_acc, 0\n",
        "            torch.save({\"state_dict\": model.state_dict(), \"val_acc\": best_val}, out_path)\n",
        "            print(f\"  ✅ Saved feature extractor → {out_path}\")\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= PATIENCE:\n",
        "                print(\"  ⛔ Early stopping.\")\n",
        "                break\n",
        "\n",
        "    ckpt = torch.load(out_path, map_location=DEVICE)\n",
        "    model.load_state_dict(ckpt[\"state_dict\"])\n",
        "    return model\n",
        "\n",
        "fe_model = train_feature_extractor(train_loader, val_loader, epochs=EPOCHS, lr=LR, out_path=MODEL_OUT)\n",
        "\n",
        "# -------- Extract embeddings (train/val) --------\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_embeddings(model, loader):\n",
        "    model.eval(); X, y = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in tqdm(loader, desc=\"Embed\"):\n",
        "            xb = xb.to(DEVICE)\n",
        "            logits, feats = model(xb, return_embedding=True)  # feats: (B, 4*hidden)\n",
        "            X.append(feats.cpu().numpy())\n",
        "            y.append(yb.numpy())\n",
        "    return np.vstack(X), np.concatenate(y)\n",
        "\n",
        "train_loader_eval = DataLoader(Subset(train_ds, train_idx), batch_size=64, shuffle=False)\n",
        "val_loader_eval   = DataLoader(Subset(val_ds,   val_idx),   batch_size=64, shuffle=False)\n",
        "\n",
        "X_tr, y_tr = extract_embeddings(fe_model, train_loader_eval)\n",
        "X_val, y_val = extract_embeddings(fe_model, val_loader_eval)\n",
        "print(\"Embeddings:\", X_tr.shape, X_val.shape)\n",
        "\n",
        "# -------- XGBoost on embeddings (binary) --------\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    n_estimators=800,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=6,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=1.0,\n",
        "    objective=\"binary:logistic\",\n",
        "    eval_metric=\"logloss\",   # <-- FIXED\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "xgb_clf.fit(X_tr, y_tr)\n",
        "\n",
        "val_preds = xgb_clf.predict(X_val)\n",
        "\n",
        "print(\"\\nCNN-LSTM Embeddings + XGBoost Results\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val, val_preds))\n",
        "print(\"Classification Report:\\n\", classification_report(y_val, val_preds))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, val_preds))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:,1]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge1cHQLUpbsN",
        "outputId": "85eb6213-4210-4092-b00f-97d01e1c5ad4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-331965069.py:158: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=torch.cuda.is_available())\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-331965069.py:178: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-331965069.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/25 | TrainLoss=6.092 | ValAcc=0.800\n",
            "  ✅ Saved feature extractor → /content/cnn_lstm_seizure.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-331965069.py:178: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-331965069.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02/25 | TrainLoss=3.122 | ValAcc=0.750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-331965069.py:178: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-331965069.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03/25 | TrainLoss=1.739 | ValAcc=0.930\n",
            "  ✅ Saved feature extractor → /content/cnn_lstm_seizure.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-331965069.py:178: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-331965069.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04/25 | TrainLoss=0.840 | ValAcc=0.990\n",
            "  ✅ Saved feature extractor → /content/cnn_lstm_seizure.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-331965069.py:178: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-331965069.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05/25 | TrainLoss=0.385 | ValAcc=0.950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-331965069.py:178: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-331965069.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06/25 | TrainLoss=0.370 | ValAcc=0.990\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-331965069.py:178: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-331965069.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07/25 | TrainLoss=0.523 | ValAcc=0.980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-331965069.py:178: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-331965069.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08/25 | TrainLoss=0.263 | ValAcc=0.980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-331965069.py:178: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-331965069.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09/25 | TrainLoss=0.283 | ValAcc=0.990\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-331965069.py:178: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "/tmp/ipython-input-331965069.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/25 | TrainLoss=0.103 | ValAcc=0.990\n",
            "  ⛔ Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embed: 100%|██████████| 7/7 [00:11<00:00,  1.61s/it]\n",
            "Embed: 100%|██████████| 2/2 [00:02<00:00,  1.22s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [11:25:02] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings: (400, 512) (100, 512)\n",
            "\n",
            "CNN-LSTM Embeddings + XGBoost Results\n",
            "Accuracy: 0.99\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99        80\n",
            "           1       1.00      0.95      0.97        20\n",
            "\n",
            "    accuracy                           0.99       100\n",
            "   macro avg       0.99      0.97      0.98       100\n",
            "weighted avg       0.99      0.99      0.99       100\n",
            "\n",
            "Confusion Matrix:\n",
            " [[80  0]\n",
            " [ 1 19]]\n",
            "ROC AUC: 0.9931249999999999\n"
          ]
        }
      ]
    }
  ]
}