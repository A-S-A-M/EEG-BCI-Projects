{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c17e411",
   "metadata": {},
   "source": [
    "1) Plan\n",
    "For each filtered EEG file:\n",
    "\n",
    "Run weak stationarity test (t-test for means, Levene for variances).\n",
    "\n",
    "Compute D₂_eff for the real signal.\n",
    "\n",
    "Generate n_surrogates IAAFT surrogates and compute D₂_eff for each; take surrogate mean ± std.\n",
    "\n",
    "Compute Nonlinear Prediction Error (NLE) using local-neighbour prediction (embedding dimension from D₂_opt whenever available).\n",
    "\n",
    "Save per-file .npz with metrics, and also accumulate a master validation_metrics.csv.\n",
    "\n",
    "After all files processed:\n",
    "\n",
    "Train a Random Forest classifier on aggregated features (D2, NLE, basic stats) as a baseline.\n",
    "\n",
    "Produce segmented .npz datasets for CNN/LSTM (options to save as .npy arrays).\n",
    "\n",
    "optimize for speed:\n",
    "\n",
    "Use joblib.Parallel to parallelize files.\n",
    "\n",
    "Use KDTree for nearest-neighbor queries.\n",
    "\n",
    "Keep surrogate count modest by default (e.g., 9) to balance speed vs. stat power — you can increase later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e20fbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [41:02<00:00,  4.92s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done. time: 3586.430608510971\n",
      "CSV saved: D:\\Downloads\\EEG_DATA_REPO\\EEG_METRICS\\validation_metrics.csv\n",
      "RF baseline accuracy (cv): mean=0.836 std=0.055\n",
      "Done. RF CV scores: [0.8  0.83 0.76 0.92 0.87]\n"
     ]
    }
   ],
   "source": [
    "# full_metrics_pipeline.py\n",
    "import os\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import ttest_ind, levene, linregress\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------------------\n",
    "# USER CONFIG\n",
    "# -------------------------\n",
    "INPUT_ROOT = r\"D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\"   # filtered files: F/,N/,O/,S/,Z/\n",
    "OUTPUT_ROOT = r\"D:\\Downloads\\EEG_DATA_REPO\\EEG_METRICS\"      # per-file npz outputs\n",
    "CSV_OUT = os.path.join(OUTPUT_ROOT, \"validation_metrics.csv\")\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "\n",
    "# D2/NLE params\n",
    "MAX_M = 12\n",
    "TAU = 1\n",
    "THEILER = 5\n",
    "EPS_POINTS = 25\n",
    "MIN_SCALING_PTS = 5\n",
    "N_NEIGHBORS_NLE = 5\n",
    "\n",
    "# Surrogates\n",
    "N_SURROGATES = 7            # default; increase to 19 for more power\n",
    "IAAFT_MAX_ITERS = 100\n",
    "\n",
    "# Parallel\n",
    "N_JOBS = -1                # use all cores\n",
    "\n",
    "# -------------------------\n",
    "# Helper functions\n",
    "# -------------------------\n",
    "def weak_stationarity_test(signal, segments=4):\n",
    "    N = len(signal)\n",
    "    seg_len = N // segments\n",
    "    if seg_len < 10:\n",
    "        return {'mean_p_vals': [], 'var_p_vals': [], 'stationary': False}\n",
    "    segs = [signal[i*seg_len:(i+1)*seg_len] for i in range(segments)]\n",
    "    p_mean = []\n",
    "    p_var = []\n",
    "    for i in range(segments):\n",
    "        for j in range(i+1, segments):\n",
    "            _, p_m = ttest_ind(segs[i], segs[j], equal_var=False)\n",
    "            _, p_v = levene(segs[i], segs[j])\n",
    "            p_mean.append(p_m)\n",
    "            p_var.append(p_v)\n",
    "    stationary = (np.min(p_mean) > 0.05) and (np.min(p_var) > 0.05)\n",
    "    return {'mean_p_vals': p_mean, 'var_p_vals': p_var, 'stationary': stationary}\n",
    "\n",
    "def embed_time_series(x, m, tau=TAU):\n",
    "    N = len(x)\n",
    "    M = N - (m - 1) * tau\n",
    "    if M <= 0:\n",
    "        return np.empty((0, m))\n",
    "    return np.array([x[i:i + m * tau:tau] for i in range(M)])\n",
    "\n",
    "def correlation_sum_kdtree(X, epsilons, theiler=THEILER):\n",
    "    N = X.shape[0]\n",
    "    tree = KDTree(X)\n",
    "    C = np.zeros(len(epsilons), dtype=np.float64)\n",
    "    for idx, eps in enumerate(epsilons):\n",
    "        neighs = tree.query_radius(X, r=eps, return_distance=False)\n",
    "        count = 0\n",
    "        for i, neigh in enumerate(neighs):\n",
    "            count += np.sum(np.abs(neigh - i) > theiler)\n",
    "        C[idx] = 2.0 * count / (N * (N - 1))\n",
    "    return C\n",
    "\n",
    "def find_scaling_region(log_eps, log_C, min_pts=MIN_SCALING_PTS):\n",
    "    max_r2 = 0.0\n",
    "    best_slope = None\n",
    "    best_range = None\n",
    "    L = len(log_eps)\n",
    "    for start in range(0, L - min_pts):\n",
    "        for end in range(start + min_pts, L):\n",
    "            x = log_eps[start:end]\n",
    "            y = log_C[start:end]\n",
    "            slope, _, r_value, _, _ = linregress(x, y)\n",
    "            if (r_value**2) > max_r2 and 0.1 < slope < 50:\n",
    "                max_r2 = r_value**2\n",
    "                best_slope = slope\n",
    "                best_range = (start, end)\n",
    "    return best_slope, best_range, max_r2\n",
    "\n",
    "def estimate_d2_eff_signal(x, m_max=MAX_M, tau=TAU, theiler=THEILER, eps_points=EPS_POINTS):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    N = len(x)\n",
    "    if N < 50:\n",
    "        return {'D2_eff': np.nan, 'm_opt': None, 'log_eps': None, 'log_C': None, 'scaling_range': None, 'valid': False}\n",
    "    s = np.std(x)\n",
    "    epsilons = np.logspace(np.log10(max(1e-12, 0.01*s)), np.log10(max(1e-12, 0.5*s)), num=eps_points)\n",
    "    log_eps = np.log10(epsilons)\n",
    "    best_slope = np.inf\n",
    "    best_m = None\n",
    "    best_log_C = None\n",
    "    best_range = None\n",
    "    max_m = min(m_max, (N - 1) // tau)\n",
    "    for m in range(1, max_m + 1):\n",
    "        X = embed_time_series(x, m, tau)\n",
    "        if X.shape[0] < 10:\n",
    "            continue\n",
    "        C = correlation_sum_kdtree(X, epsilons, theiler=theiler)\n",
    "        log_C = np.log10(C + 1e-12)\n",
    "        slope, eps_range, r2 = find_scaling_region(log_eps, log_C)\n",
    "        if slope is not None and slope < best_slope:\n",
    "            best_slope = slope\n",
    "            best_m = m\n",
    "            best_log_C = log_C\n",
    "            best_range = eps_range\n",
    "    valid = best_m is not None\n",
    "    return {'D2_eff': float(best_slope) if valid else np.nan,\n",
    "            'm_opt': int(best_m) if valid else None,\n",
    "            'log_eps': log_eps if valid else None,\n",
    "            'log_C': best_log_C if valid else None,\n",
    "            'scaling_range': best_range if valid else None,\n",
    "            'valid': bool(valid)}\n",
    "\n",
    "# -------- IAAFT surrogate generator (vectorized-ish) ----------\n",
    "def iaaft_surrogate(x, max_iter=IAAFT_MAX_ITERS):\n",
    "    # Single surrogate; returns same-length vector\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    N = len(x)\n",
    "    sorted_x = np.sort(x)\n",
    "    # original amplitude spectrum\n",
    "    orig_fft = np.fft.rfft(x)\n",
    "    amp = np.abs(orig_fft)\n",
    "    # initialize surrogate as random shuffle\n",
    "    surr = np.random.permutation(x)\n",
    "    for _ in range(max_iter):\n",
    "        # impose target spectrum\n",
    "        surr_fft = np.fft.rfft(surr)\n",
    "        phases = np.angle(surr_fft)\n",
    "        surr = np.fft.irfft(amp * np.exp(1j * phases), n=N)\n",
    "        # impose amplitude distribution by rank-order\n",
    "        surr = sorted_x[np.argsort(np.argsort(surr))]\n",
    "    return surr\n",
    "\n",
    "def make_iaaft_surrogates(x, n_surrogates=7):\n",
    "    return [iaaft_surrogate(x) for _ in range(n_surrogates)]\n",
    "\n",
    "# -------- NLE ----------\n",
    "def nonlinear_prediction_error(x, m=3, tau=TAU, k=N_NEIGHBORS_NLE):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    X = embed_time_series(x, m+1, tau)\n",
    "    if X.shape[0] < (k+3):\n",
    "        return np.nan\n",
    "    X_obs = X[:, :-1]\n",
    "    y = X[:, -1]\n",
    "    tree = KDTree(X_obs)\n",
    "    dists, idxs = tree.query(X_obs, k=k+1)\n",
    "    preds = []\n",
    "    for i in range(X_obs.shape[0]):\n",
    "        neigh_idxs = idxs[i, 1:]  # exclude self\n",
    "        pred = np.mean(X[neigh_idxs, -1])\n",
    "        preds.append(pred)\n",
    "    preds = np.array(preds)\n",
    "    return float(np.sqrt(mean_squared_error(y, preds)))\n",
    "\n",
    "# -------------------------\n",
    "# Per-file worker\n",
    "# -------------------------\n",
    "def process_file(in_path, out_dir, compute_surrogates=True):\n",
    "    name = os.path.splitext(os.path.basename(in_path))[0]\n",
    "    out_path = os.path.join(out_dir, f\"{name}.npz\")\n",
    "    if os.path.exists(out_path):\n",
    "        return {'name': name, 'status': 'exists'}\n",
    "\n",
    "    try:\n",
    "        x = np.loadtxt(in_path)\n",
    "    except Exception as e:\n",
    "        return {'name': name, 'status': 'load_error', 'error': str(e)}\n",
    "\n",
    "    # 1. stationarity\n",
    "    stat = weak_stationarity_test(x)\n",
    "\n",
    "    # 2. D2 for real\n",
    "    d2_real = estimate_d2_eff_signal(x)\n",
    "\n",
    "    # 3. D2 for surrogates (mean+std)\n",
    "    surr_mean = np.nan\n",
    "    surr_std = np.nan\n",
    "    if compute_surrogates:\n",
    "        sur_d2s = []\n",
    "        for s in make_iaaft_surrogates(x, n_surrogates=N_SURROGATES):\n",
    "            dd = estimate_d2_eff_signal(s)\n",
    "            sur_d2s.append(dd['D2_eff'])\n",
    "        sur_d2s = np.array(sur_d2s, dtype=np.float64)\n",
    "        surr_mean = float(np.nanmean(sur_d2s))\n",
    "        surr_std = float(np.nanstd(sur_d2s))\n",
    "\n",
    "    # 4. NLE (use d2 m_opt if available)\n",
    "    m_for_nle = d2_real['m_opt'] if d2_real['m_opt'] is not None else 3\n",
    "    nle = nonlinear_prediction_error(x, m=max(2, m_for_nle), tau=TAU, k=N_NEIGHBORS_NLE)\n",
    "\n",
    "    # 5. basic stats\n",
    "    basic = {'mean': float(np.mean(x)), 'std': float(np.std(x)),\n",
    "             'min': float(np.min(x)), 'max': float(np.max(x))}\n",
    "\n",
    "    np.savez_compressed(out_path,\n",
    "                        name=name,\n",
    "                        stationarity=stat,\n",
    "                        d2_real=d2_real,\n",
    "                        d2_surr_mean=surr_mean,\n",
    "                        d2_surr_std=surr_std,\n",
    "                        nle=nle,\n",
    "                        basic=basic)\n",
    "    return {'name': name, 'status': 'processed', 'd2_real': d2_real['D2_eff'], 'd2_surr_mean': surr_mean, 'nle': nle}\n",
    "\n",
    "# -------------------------\n",
    "# Run across all files in parallel\n",
    "# -------------------------\n",
    "def run_all(input_root=INPUT_ROOT, output_root=OUTPUT_ROOT, labels=['F','N','O','S','Z'], compute_surrogates=True):\n",
    "    tasks = []\n",
    "    for label in labels:\n",
    "        in_folder = os.path.join(input_root, label)\n",
    "        out_folder = os.path.join(output_root, label)\n",
    "        os.makedirs(out_folder, exist_ok=True)\n",
    "        for fname in sorted(os.listdir(in_folder)):\n",
    "            if fname.lower().endswith('.txt') or fname.lower().endswith('.npy'):\n",
    "                tasks.append((os.path.join(in_folder, fname), out_folder))\n",
    "\n",
    "    print(\"Total files:\", len(tasks))\n",
    "    results = Parallel(n_jobs=N_JOBS)(\n",
    "        delayed(process_file)(t[0], t[1], compute_surrogates) for t in tqdm(tasks)\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# -------------------------\n",
    "# Aggregate to CSV\n",
    "# -------------------------\n",
    "import csv\n",
    "def aggregate_to_csv(metrics_root=OUTPUT_ROOT, csv_out=CSV_OUT, labels=['F','N','O','S','Z']):\n",
    "    rows = []\n",
    "    for label in labels:\n",
    "        folder = os.path.join(metrics_root, label)\n",
    "        if not os.path.exists(folder): continue\n",
    "        for fname in sorted(os.listdir(folder)):\n",
    "            if not fname.endswith('.npz'): continue\n",
    "            data = np.load(os.path.join(folder, fname), allow_pickle=True)\n",
    "            name = data['name'].item()\n",
    "            stationarity = data['stationarity'].item()\n",
    "            d2r = data['d2_real'].item()['D2_eff'] if isinstance(data['d2_real'].item(), dict) else float(data['d2_real'].item())\n",
    "            d2s_mean = float(data['d2_surr_mean'].item())\n",
    "            nle = float(data['nle'].item())\n",
    "            basic = data['basic'].item()\n",
    "            rows.append({'label': label, 'name': name, 'stationary': stationarity['stationary'],\n",
    "                         'd2_real': d2r, 'd2_surr_mean': d2s_mean, 'd2_surr_std': float(data['d2_surr_std'].item()),\n",
    "                         'nle': nle,\n",
    "                         'mean': basic['mean'], 'std': basic['std']})\n",
    "    # write csv\n",
    "    keys = ['label','name','stationary','d2_real','d2_surr_mean','d2_surr_std','nle','mean','std']\n",
    "    with open(csv_out, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        for r in rows:\n",
    "            writer.writerow(r)\n",
    "    print(\"CSV saved:\", csv_out)\n",
    "    return csv_out\n",
    "\n",
    "# -------------------------\n",
    "# Random forest baseline training\n",
    "# -------------------------\n",
    "def rf_baseline_from_csv(csv_path=CSV_OUT, n_splits=5):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # drop NaNs, simple imputation if needed\n",
    "    df = df.dropna()\n",
    "    X = df[['d2_real','d2_surr_mean','nle','mean','std']].values\n",
    "    y = df['label'].values\n",
    "    # simple label encoding\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "    rf = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42)\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(rf, X, y_enc, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    print(\"RF baseline accuracy (cv): mean=%.3f std=%.3f\" % (scores.mean(), scores.std()))\n",
    "    return rf, scores, le\n",
    "\n",
    "# -------------------------\n",
    "# Example run in __main__\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    t0 = time.time()\n",
    "    results = run_all(compute_surrogates=True)\n",
    "    print(\"Processing done. time:\", time.time() - t0)\n",
    "    aggregate_to_csv()\n",
    "    rf, scores, le = rf_baseline_from_csv()\n",
    "    print(\"Done. RF CV scores:\", scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccc44b",
   "metadata": {},
   "source": [
    "Next steps for CNN / LSTM pipelines:\n",
    "\n",
    "Prepare your input data:\n",
    "\n",
    "For CNNs, you typically need fixed-length windows of your EEG signals (e.g., 4096 points per window) and optionally normalized.\n",
    "\n",
    "For LSTMs, you can feed raw sequential data or preprocessed features (D2, NLE, etc.) in time order.\n",
    "\n",
    "Label encoding:\n",
    "\n",
    "Your labels (F, N, O, S, Z) need to be numerically encoded for supervised learning.\n",
    "\n",
    "Train-test split:\n",
    "\n",
    "Keep some portion of the data for validation/testing to avoid overfitting.\n",
    "\n",
    "CNN pipeline:\n",
    "\n",
    "Input: 1D EEG signal (shape: [samples, timesteps, 1])\n",
    "\n",
    "Layers: Conv1D → ReLU → Pooling → Flatten → Dense → Softmax\n",
    "\n",
    "Loss: Categorical Cross-Entropy\n",
    "\n",
    "Optimizer: Adam\n",
    "\n",
    "LSTM pipeline:\n",
    "\n",
    "Input: [samples, timesteps, features]\n",
    "\n",
    "Layers: LSTM → Dropout → Dense → Softmax\n",
    "\n",
    "Loss: Categorical Cross-Entropy\n",
    "\n",
    "Optimizer: Adam\n",
    "\n",
    "Evaluation:\n",
    "\n",
    "Accuracy, confusion matrix, F1 score.\n",
    "\n",
    "Compare CNN/LSTM performance to your Random Forest baseline (≈ 0.836)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4fb9e3",
   "metadata": {},
   "source": [
    "4) How to run & expected runtime\n",
    "\n",
    "Put the script in the same environment you used for filtering (it needs numpy, joblib, scipy, sklearn, tqdm).\n",
    "\n",
    "Default N_SURROGATES = 7. With D₂ computations\n",
    "and surrogates, expect this to take minutes per file unless signals are short. Parallelism reduces wall-clock time. For 500 signals, runtime could be several hours depending on CPU. If you want speed:\n",
    "\n",
    "Lower N_SURROGATES (1–3) for a quick check.\n",
    "\n",
    "Lower MAX_M to 8.\n",
    "\n",
    "Or run on a machine with more CPU cores.\n",
    "\n",
    "The script saves per-file .npz — you can resume by re-running; it skips files if .npz already exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0defce26",
   "metadata": {},
   "source": [
    "the 'N' folder is empty or missing those .txt files altogether under EEGDATA_EXTRACTED.\n",
    "\n",
    "That means the problem happened at or before extraction from the zip files.\n",
    "\n",
    "Next immediate steps:\n",
    "\n",
    "Check the raw zip for 'N' files:\n",
    "\n",
    "Look inside the original N.zip to verify it actually contains .txt files.\n",
    "\n",
    "Run this snippet to list contents without extracting:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af3bb8a",
   "metadata": {},
   "source": [
    "After running — what to inspect\n",
    "\n",
    "EEG_METRICS/<label>/file.npz — open few with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4de7152e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'EEG_METRICS/F/S01.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m d = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEEG_METRICS/F/S01.npz\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(d[\u001b[33m'\u001b[39m\u001b[33md2_real\u001b[39m\u001b[33m'\u001b[39m].item())\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNLE:\u001b[39m\u001b[33m\"\u001b[39m, d[\u001b[33m'\u001b[39m\u001b[33mnle\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\lib\\_npyio_impl.py:454\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    452\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     fid = stack.enter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    455\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'EEG_METRICS/F/S01.npz'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 12/400 [00:20<00:07, 52.42it/s]"
     ]
    }
   ],
   "source": [
    "d = np.load(\"EEG_METRICS/F/S01.npz\", allow_pickle=True)\n",
    "print(d['d2_real'].item())\n",
    "print(\"NLE:\", d['nle'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0630a87c",
   "metadata": {},
   "source": [
    "validation_metrics.csv — quick view of real vs surrogate D₂_mean and NLE distribution.\n",
    "\n",
    "RF baseline printed accuracy — gives simple baseline comparing feature-based method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3f2c47",
   "metadata": {},
   "source": [
    "6) Next steps I can do for you (pick any)\n",
    "\n",
    "Increase surrogate count and/or compute p-values per-file (e.g., real D₂ > 95th percentile of surrogates).\n",
    "\n",
    "Produce visualization code: histograms, boxplots of D₂_real vs D₂_surr across labels.\n",
    "\n",
    "Build and train full CNN and LSTM on segmented raw data created from filtered signals (I’ll produce the segmentation & model scripts).\n",
    "\n",
    "Tune RF classifier (feature selection, hyperparameter search) or try XGBoost.\n",
    "\n",
    "If you’d like, I can now:\n",
    "\n",
    "paste the script into your environment and walk you through running it step-by-step, OR\n",
    "\n",
    "immediately produce the CNN/LSTM dataset & model scripts (with training/eval loops) after this pipeline finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93fcd468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files inside N.zip:\n",
      "['N001.TXT', 'N099.TXT', 'N098.TXT', 'N097.TXT', 'N096.TXT', 'N095.TXT', 'N094.TXT', 'N093.TXT', 'N092.TXT', 'N091.TXT', 'N090.TXT', 'N089.TXT', 'N088.TXT', 'N087.TXT', 'N086.TXT', 'N085.TXT', 'N084.TXT', 'N083.TXT', 'N082.TXT', 'N081.TXT', 'N080.TXT', 'N079.TXT', 'N078.TXT', 'N077.TXT', 'N076.TXT', 'N075.TXT', 'N074.TXT', 'N073.TXT', 'N072.TXT', 'N071.TXT', 'N070.TXT', 'N069.TXT', 'N068.TXT', 'N067.TXT', 'N066.TXT', 'N065.TXT', 'N064.TXT', 'N063.TXT', 'N062.TXT', 'N061.TXT', 'N060.TXT', 'N059.TXT', 'N058.TXT', 'N057.TXT', 'N056.TXT', 'N055.TXT', 'N054.TXT', 'N053.TXT', 'N052.TXT', 'N051.TXT', 'N050.TXT', 'N049.TXT', 'N048.TXT', 'N047.TXT', 'N046.TXT', 'N045.TXT', 'N044.TXT', 'N043.TXT', 'N042.TXT', 'N041.TXT', 'N040.TXT', 'N039.TXT', 'N038.TXT', 'N037.TXT', 'N036.TXT', 'N035.TXT', 'N034.TXT', 'N033.TXT', 'N032.TXT', 'N031.TXT', 'N030.TXT', 'N029.TXT', 'N028.TXT', 'N027.TXT', 'N026.TXT', 'N025.TXT', 'N024.TXT', 'N023.TXT', 'N022.TXT', 'N021.TXT', 'N020.TXT', 'N019.TXT', 'N018.TXT', 'N017.TXT', 'N016.TXT', 'N015.TXT', 'N014.TXT', 'N013.TXT', 'N012.TXT', 'N011.TXT', 'N010.TXT', 'N009.TXT', 'N008.TXT', 'N007.TXT', 'N006.TXT', 'N005.TXT', 'N004.TXT', 'N003.TXT', 'N002.TXT', 'N100.TXT']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_path = r\"D:\\Downloads\\EEG_DATA_REPO\\EEGDATA\\N.zip\"\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    print(\"Files inside N.zip:\")\n",
    "    print(zip_ref.namelist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44287440",
   "metadata": {},
   "source": [
    "If the zip is empty or missing expected .txt files, you’ll need to re-download or source a proper archive.\n",
    "\n",
    "If files are present in zip, re-extract carefully:\n",
    "\n",
    "Make sure you are extracting the zip correctly. Here’s a robust extract snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e2c47d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted N.zip contents to: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_EXTRACTED\\N\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = r\"D:\\Downloads\\EEG_DATA_REPO\\EEGDATA\\N.zip\"\n",
    "extract_dir = r\"D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_EXTRACTED\\N\"\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(f\"Extracted N.zip contents to: {extract_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a679aa43",
   "metadata": {},
   "source": [
    "After extraction, check the folder again:\n",
    "\n",
    "in EEGDATA_EXTRACTED THE N files were written as N001.TXT and so on. we need to re convert them to N....txt and run the d2eff and the steps on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "036e2b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n001.txt', 'n002.txt', 'n003.txt', 'n004.txt', 'n005.txt', 'n006.txt', 'n007.txt', 'n008.txt', 'n009.txt', 'n010.txt', 'n011.txt', 'n012.txt', 'n013.txt', 'n014.txt', 'n015.txt', 'n016.txt', 'n017.txt', 'n018.txt', 'n019.txt', 'n020.txt', 'n021.txt', 'n022.txt', 'n023.txt', 'n024.txt', 'n025.txt', 'n026.txt', 'n027.txt', 'n028.txt', 'n029.txt', 'n030.txt', 'n031.txt', 'n032.txt', 'n033.txt', 'n034.txt', 'n035.txt', 'n036.txt', 'n037.txt', 'n038.txt', 'n039.txt', 'n040.txt', 'n041.txt', 'n042.txt', 'n043.txt', 'n044.txt', 'n045.txt', 'n046.txt', 'n047.txt', 'n048.txt', 'n049.txt', 'n050.txt', 'n051.txt', 'n052.txt', 'n053.txt', 'n054.txt', 'n055.txt', 'n056.txt', 'n057.txt', 'n058.txt', 'n059.txt', 'n060.txt', 'n061.txt', 'n062.txt', 'n063.txt', 'n064.txt', 'n065.txt', 'n066.txt', 'n067.txt', 'n068.txt', 'n069.txt', 'n070.txt', 'n071.txt', 'n072.txt', 'n073.txt', 'n074.txt', 'n075.txt', 'n076.txt', 'n077.txt', 'n078.txt', 'n079.txt', 'n080.txt', 'n081.txt', 'n082.txt', 'n083.txt', 'n084.txt', 'n085.txt', 'n086.txt', 'n087.txt', 'n088.txt', 'n089.txt', 'n090.txt', 'n091.txt', 'n092.txt', 'n093.txt', 'n094.txt', 'n095.txt', 'n096.txt', 'n097.txt', 'n098.txt', 'n099.txt', 'n100.txt']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(extract_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165cfaa",
   "metadata": {},
   "source": [
    "Your filtering and analysis code is looking for files ending with .txt (lowercase) — so N001.TXT (uppercase) won't be picked up.\n",
    "\n",
    "Also, you might want consistent filenames (N001.txt) to keep your pipeline smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "310fb793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "n_dir = r\"D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_EXTRACTED\\N\"\n",
    "\n",
    "for fname in os.listdir(n_dir):\n",
    "    if fname.endswith('.TXT'):\n",
    "        old_path = os.path.join(n_dir, fname)\n",
    "        new_fname = fname.lower()  # e.g., N001.TXT -> n001.txt\n",
    "        new_path = os.path.join(n_dir, new_fname)\n",
    "        os.rename(old_path, new_path)\n",
    "        print(f\"Renamed: {fname} -> {new_fname}\")\n",
    "print(\"Renaming complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c4d55e",
   "metadata": {},
   "source": [
    "After renaming:\n",
    "\n",
    "Your filtering and subsequent steps will find and process the N channel files seamlessly.\n",
    "\n",
    "Make sure your scripts are case-insensitive when reading files or that you consistently lowercase filenames.\n",
    "\n",
    "Optional improvement:\n",
    "\n",
    "If you want, I can help you update the file loading code to automatically accept any case (.TXT or .txt) to avoid this issue in the future.\n",
    "\n",
    "Once renamed, just rerun your filtering, feature extraction, and classification pipeline including the N files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9740cdeb",
   "metadata": {},
   "source": [
    "Here’s a Python script to verify all your zip files for presence of .txt files and then extract only those missing or incomplete into your EEGDATA_EXTRACTED folder. This way you won't miss any channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9617f738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking F...\n",
      "  ✅ Extraction folder already has all files for F, skipping extraction.\n",
      "Checking N...\n",
      "  ⚠️ WARNING: No .txt files found inside N.zip!\n",
      "Checking O...\n",
      "  ✅ Extraction folder already has all files for O, skipping extraction.\n",
      "Checking S...\n",
      "  ✅ Extraction folder already has all files for S, skipping extraction.\n",
      "Checking Z...\n",
      "  ✅ Extraction folder already has all files for Z, skipping extraction.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_root = r\"D:\\Downloads\\EEG_DATA_REPO\\EEGDATA\"\n",
    "extract_root = r\"D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_EXTRACTED\"\n",
    "\n",
    "channels = ['F', 'N', 'O', 'S', 'Z']\n",
    "\n",
    "for ch in channels:\n",
    "    zip_path = os.path.join(zip_root, f\"{ch}.zip\")\n",
    "    extract_dir = os.path.join(extract_root, ch)\n",
    "\n",
    "    print(f\"Checking {ch}...\")\n",
    "\n",
    "    # Check zip contents\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        txt_files_in_zip = [f for f in zip_ref.namelist() if f.endswith('.txt')]\n",
    "        if not txt_files_in_zip:\n",
    "            print(f\"  ⚠️ WARNING: No .txt files found inside {ch}.zip!\")\n",
    "            continue\n",
    "\n",
    "    # Check if extraction folder exists and has .txt files\n",
    "    if os.path.exists(extract_dir):\n",
    "        extracted_txt_files = [f for f in os.listdir(extract_dir) if f.endswith('.txt')]\n",
    "    else:\n",
    "        extracted_txt_files = []\n",
    "\n",
    "    # Decide whether to extract or skip\n",
    "    if len(extracted_txt_files) >= len(txt_files_in_zip):\n",
    "        print(f\"  ✅ Extraction folder already has all files for {ch}, skipping extraction.\")\n",
    "    else:\n",
    "        print(f\"  ⏳ Extracting {ch}.zip to {extract_dir} ...\")\n",
    "        os.makedirs(extract_dir, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_dir)\n",
    "        print(f\"  ✅ Extraction done for {ch}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93806d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 N files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering N files: 100%|██████████| 100/100 [00:00<00:00, 114.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All N files filtered & saved in D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, filtfilt\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "EXTRACTED_DIR =  r\"D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_EXTRACTED\\N\"\n",
    "FILTERED_DIR = r\"D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\"\n",
    "\n",
    "os.makedirs(FILTERED_DIR, exist_ok=True)\n",
    "\n",
    "# --- Bandpass Filter ---\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype=\"band\")\n",
    "    return b, a\n",
    "\n",
    "def bandpass_filter(data, lowcut=0.5, highcut=40.0, fs=173.61, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "# --- Load and filter N files ---\n",
    "def process_N_files():\n",
    "    all_files = [f for f in os.listdir(EXTRACTED_DIR) if f.startswith(\"n\") and f.endswith(\".txt\")]\n",
    "    \n",
    "    print(f\"Found {len(all_files)} N files\")\n",
    "    for fname in tqdm(all_files, desc=\"Filtering N files\"):\n",
    "        filepath = os.path.join(EXTRACTED_DIR, fname)\n",
    "        \n",
    "        # Load data\n",
    "        data = np.loadtxt(filepath)\n",
    "        \n",
    "        # Apply bandpass filter\n",
    "        filtered = bandpass_filter(data, fs=173.61)\n",
    "        \n",
    "        # Save filtered file\n",
    "        savepath = os.path.join(FILTERED_DIR, fname.replace(\".txt\", \".txt\"))\n",
    "        np.savetxt(savepath, filtered)\n",
    "\n",
    "    print(f\"✅ All N files filtered & saved in {FILTERED_DIR}\")\n",
    "\n",
    "# --- Run ---\n",
    "process_N_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6ee3cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 files to delete.\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n001_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n002_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n003_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n004_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n005_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n006_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n007_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n008_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n009_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n010_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n011_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n012_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n013_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n014_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n015_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n016_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n017_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n018_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n019_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n020_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n021_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n022_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n023_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n024_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n025_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n026_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n027_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n028_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n029_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n030_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n031_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n032_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n033_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n034_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n035_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n036_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n037_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n038_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n039_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n040_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n041_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n042_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n043_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n044_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n045_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n046_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n047_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n048_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n049_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n050_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n051_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n052_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n053_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n054_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n055_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n056_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n057_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n058_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n059_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n060_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n061_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n062_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n063_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n064_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n065_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n066_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n067_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n068_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n069_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n070_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n071_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n072_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n073_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n074_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n075_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n076_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n077_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n078_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n079_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n080_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n081_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n082_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n083_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n084_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n085_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n086_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n087_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n088_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n089_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n090_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n091_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n092_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n093_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n094_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n095_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n096_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n097_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n098_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n099_filtered.txt\n",
      "Deleted: D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\\n100_filtered.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Path to the N folder inside EEGDATA_FILTERED\n",
    "folder = r\"D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\"\n",
    "\n",
    "# Pattern: n + 4 characters + _filtered.txt\n",
    "pattern = os.path.join(folder, \"n???_filtered.txt\")\n",
    "\n",
    "# Find matching files\n",
    "files_to_delete = glob.glob(pattern)\n",
    "\n",
    "print(f\"Found {len(files_to_delete)} files to delete.\")\n",
    "\n",
    "# Delete them\n",
    "for file in files_to_delete:\n",
    "    try:\n",
    "        os.remove(file)\n",
    "        print(f\"Deleted: {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "361de2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# Path to N folder inside EEGDATA_FILTERED\n",
    "folder = \"EEGDATA_FILTERED/N\"\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for file in os.listdir(folder):\n",
    "    if \"_filtered.txt\" in file:   # only match the unwanted files\n",
    "        file_path = os.path.join(folder, file)\n",
    "        os.remove(file_path)      # delete the file\n",
    "        print(f\"Deleted: {file_path}\")\n",
    "        time.sleep(0.1)  # small delay for safety (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7a8ad7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: ' D:\\\\Downloads\\\\EEG_DATA_REPO\\\\EEGDATA_FILTERED\\\\N\\\\validation_metrics.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m csv_n = csv_n.strip()\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Load both\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m df_main = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_main\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m df_n    = pd.read_csv(csv_n)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Compare or merge as needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mOSError\u001b[39m: [Errno 22] Invalid argument: ' D:\\\\Downloads\\\\EEG_DATA_REPO\\\\EEGDATA_FILTERED\\\\N\\\\validation_metrics.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define your output root (replace with your actual path)\n",
    "OUTPUT_ROOT = r\" D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\\N\"   # <-- update this\n",
    "\n",
    "# Paths to metrics files\n",
    "csv_main = os.path.join(OUTPUT_ROOT, \"validation_metrics.csv\")\n",
    "csv_n    = os.path.join(OUTPUT_ROOT, \"metrics_N.csv\")\n",
    "\n",
    "csv_n = csv_n.strip()\n",
    "\n",
    "# Load both\n",
    "df_main = pd.read_csv(csv_main)\n",
    "df_n    = pd.read_csv(csv_n)\n",
    "\n",
    "# Compare or merge as needed\n",
    "print(df_main.head())\n",
    "print(df_n.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2faeac6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m t0 = time.time()\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# run only for N\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m results = \u001b[43mrun_all\u001b[49m(labels=[\u001b[33m'\u001b[39m\u001b[33mN\u001b[39m\u001b[33m'\u001b[39m], compute_surrogates=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProcessing done. time:\u001b[39m\u001b[33m\"\u001b[39m, time.time() - t0)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# aggregate only N into a temporary CSV\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'run_all' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    t0 = time.time()\n",
    "    # run only for N\n",
    "    results = run_all(labels=['N'], compute_surrogates=True)\n",
    "    print(\"Processing done. time:\", time.time() - t0)\n",
    "\n",
    "    # aggregate only N into a temporary CSV\n",
    "    aggregate_to_csv(labels=['N'], csv_out=os.path.join(OUTPUT_ROOT, \"metrics_N.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc80372b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming complete. Total files renamed: 0\n",
      "Total N files to filter: 100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# === Step 1: Rename .TXT to .txt in 'N' folder ===\n",
    "def rename_uppercase_txt_to_lower(directory):\n",
    "    renamed_files = 0\n",
    "    for fname in os.listdir(directory):\n",
    "        if fname.endswith('.TXT'):\n",
    "            old_path = os.path.join(directory, fname)\n",
    "            new_fname = fname.lower()\n",
    "            new_path = os.path.join(directory, new_fname)\n",
    "            os.rename(old_path, new_path)\n",
    "            renamed_files += 1\n",
    "            print(f\"Renamed: {fname} -> {new_fname}\")\n",
    "    print(f\"Renaming complete. Total files renamed: {renamed_files}\")\n",
    "\n",
    "# === Step 2: Filtering setup ===\n",
    "Fs = 173.16\n",
    "Fc = 40\n",
    "order = 4\n",
    "Wn = Fc / (Fs / 2)\n",
    "b, a = butter(order, Wn, btype='low', analog=False)\n",
    "\n",
    "def filter_and_save(task):\n",
    "    in_path, out_path = task\n",
    "    signal = np.loadtxt(in_path)\n",
    "    filtered = filtfilt(b, a, signal)\n",
    "    np.savetxt(out_path, filtered)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# === Step 3: Run filtering only for N ===\n",
    "def run_filtering_for_N(input_root, output_root):\n",
    "    input_dir = os.path.join(input_root, 'N')\n",
    "    output_dir = os.path.join(output_root, 'N')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    tasks = []\n",
    "    for fname in sorted(os.listdir(input_dir)):\n",
    "        if fname.lower().endswith('.txt'):\n",
    "            in_path = os.path.join(input_dir, fname)\n",
    "            out_path = os.path.join(output_dir, fname.lower())\n",
    "            tasks.append((in_path, out_path))\n",
    "\n",
    "    print(f\"Total N files to filter: {len(tasks)}\")\n",
    "\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        for result in pool.imap_unordered(filter_and_save, tasks):\n",
    "            print(f\"Saved: {result}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extracted_root = r\"D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_EXTRACTED\"\n",
    "    filtered_root  = r\"D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\"\n",
    "\n",
    "    # Step 1: Fix .TXT case problem in N folder\n",
    "    rename_uppercase_txt_to_lower(os.path.join(extracted_root, 'N'))\n",
    "\n",
    "    # Step 2: Run filtering only for N\n",
    "    run_filtering_for_N(extracted_root, filtered_root)\n",
    "\n",
    "    print(\"✅ Filtering complete for N channel only!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8de607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming complete. Total files renamed: 0\n",
      "Total files to filter: 500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# === Step 1: Rename .TXT to .txt in 'N' folder ===\n",
    "def rename_uppercase_txt_to_lower(directory):\n",
    "    renamed_files = 0\n",
    "    for fname in os.listdir(directory):\n",
    "        if fname.endswith('.TXT'):\n",
    "            old_path = os.path.join(directory, fname)\n",
    "            new_fname = fname.lower()\n",
    "            new_path = os.path.join(directory, new_fname)\n",
    "            os.rename(old_path, new_path)\n",
    "            renamed_files += 1\n",
    "            print(f\"Renamed: {fname} -> {new_fname}\")\n",
    "    print(f\"Renaming complete. Total files renamed: {renamed_files}\")\n",
    "\n",
    "# === Step 2: Filtering function ===\n",
    "Fs = 173.16\n",
    "Fc = 40\n",
    "order = 4\n",
    "Wn = Fc / (Fs / 2)\n",
    "b, a = butter(order, Wn, btype='low', analog=False)\n",
    "\n",
    "def filter_and_save(task):\n",
    "    in_path, out_path = task\n",
    "    signal = np.loadtxt(in_path)\n",
    "    filtered = filtfilt(b, a, signal)\n",
    "    np.savetxt(out_path, filtered)\n",
    "    return out_path\n",
    "\n",
    "# === Step 3: Prepare and run filtering on all channels ===\n",
    "def run_filtering(input_root, output_root, channels):\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "    tasks = []\n",
    "\n",
    "    for ch in channels:\n",
    "        input_dir = os.path.join(input_root, ch)\n",
    "        output_dir = os.path.join(output_root, ch)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for fname in sorted(os.listdir(input_dir)):\n",
    "            # Accept .txt or .TXT but after renaming ideally .txt only\n",
    "            if fname.lower().endswith('.txt'):\n",
    "                in_path = os.path.join(input_dir, fname)\n",
    "                out_path = os.path.join(output_dir, fname.lower())\n",
    "                tasks.append((in_path, out_path))\n",
    "\n",
    "    print(f\"Total files to filter: {len(tasks)}\")\n",
    "\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        for result in pool.imap_unordered(filter_and_save, tasks):\n",
    "            print(f\"Saved: {result}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Change these paths to your environment\n",
    "    extracted_root = r\"D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_EXTRACTED\"\n",
    "    filtered_root = r\"D:\\Downloads\\EEG_DATA_REPO\\EEGDATA_FILTERED\"\n",
    "    channel_list = ['F', 'N', 'O', 'S', 'Z']\n",
    "\n",
    "    # Step 1: Rename in 'N'\n",
    "    rename_uppercase_txt_to_lower(os.path.join(extracted_root, 'N'))\n",
    "\n",
    "    # Step 2 & 3: Run filtering on all channels\n",
    "    run_filtering(extracted_root, filtered_root, channel_list)\n",
    "\n",
    "    print(\"✅ All filtering complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d677166d",
   "metadata": {},
   "source": [
    "Set Z (Healthy, eyes open)\n",
    "\n",
    "EEG from healthy volunteers in a resting state with eyes open.\n",
    "\n",
    "Recorded from the surface electrodes.\n",
    "\n",
    "Considered baseline normal activity.\n",
    "\n",
    "\n",
    "Set O (Healthy, eyes closed)\n",
    "\n",
    "EEG from the same healthy volunteers, but with eyes closed.\n",
    "\n",
    "Also surface recordings.\n",
    "\n",
    "Typically has stronger alpha rhythms (8–12 Hz) compared to Set Z.\n",
    "\n",
    "Set N (Interictal, opposite hemisphere)\n",
    "\n",
    "EEG from epileptic patients, recorded during seizure-free intervals (interictal state).\n",
    "\n",
    "Taken from the hippocampal formation of the opposite hemisphere (not the epileptogenic zone).\n",
    "\n",
    "Intracranial recording (depth electrode).\n",
    "\n",
    "Set F (Interictal, epileptogenic zone)\n",
    "\n",
    "EEG from epileptic patients, also in seizure-free intervals.\n",
    "\n",
    "Recorded from the epileptogenic zone (where seizures originate).\n",
    "\n",
    "Intracranial (depth electrode).\n",
    "\n",
    "Often shows subtle pathological activity.\n",
    "\n",
    "\n",
    "🔹 Set S (Seizure activity / Ictal EEG)\n",
    "\n",
    "EEG from epileptic patients during seizures (ictal periods).\n",
    "\n",
    "Intracranial recordings from within the epileptogenic zone.\n",
    "\n",
    "Contains clear seizure discharges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdc736c",
   "metadata": {},
   "source": [
    "Clinical Terms in EEG Context\n",
    "\n",
    "Ictal\n",
    "👉 Refers to the actual seizure event (from onset until termination).\n",
    "When an EEG segment is labeled ictal, it means the patient is actively experiencing a seizure in that recording window.\n",
    "\n",
    "“Ictus” = Latin for “strike/attack.”\n",
    "\n",
    "EEG shows rhythmic, abnormal discharges (spikes, sharp waves, spike-and-wave complexes).\n",
    "\n",
    "Interictal\n",
    "👉 EEG recorded between seizures.\n",
    "The patient is not seizing, but there may be abnormal background activity (epileptiform discharges like spikes or sharp waves).\n",
    "Important for diagnosis — many epilepsy patients show abnormalities even when not seizing.\n",
    "\n",
    "Preictal\n",
    "👉 The time before a seizure starts.\n",
    "EEG may begin showing gradual changes (slowing, rhythmic buildup, nonlinear shifts). Used in seizure prediction studies.\n",
    "\n",
    "Postictal\n",
    "👉 The time right after a seizure ends.\n",
    "EEG often shows slowing, suppression, or diffuse abnormalities as the brain recovers.\n",
    "\n",
    "🔹 Mapping to the Bonn Dataset Sets\n",
    "\n",
    "The Bonn dataset has 5 subsets (Z, O, F, N, S):\n",
    "\n",
    "Z – Healthy volunteers, eyes open. (Normal baseline)\n",
    "\n",
    "O – Healthy volunteers, eyes closed. (Normal baseline, less visual artifact)\n",
    "\n",
    "N – Epileptic patients, interictal, EEG recorded from the healthy hemisphere.\n",
    "\n",
    "F – Epileptic patients, interictal, EEG recorded from the epileptogenic zone (but not during seizure).\n",
    "\n",
    "S – Epileptic patients, ictal — EEG recorded during seizures.\n",
    "\n",
    "🔹 Practical Use\n",
    "\n",
    "Z & O → Normal control signals.\n",
    "\n",
    "N & F → Interictal (important for detecting abnormalities outside seizures).\n",
    "\n",
    "S → Ictal (active seizure events).\n",
    "\n",
    "So in summary:\n",
    "\n",
    "Ictal = seizure,\n",
    "\n",
    "Interictal = between seizures,\n",
    "\n",
    "Preictal = before seizure,\n",
    "\n",
    "Postictal = after seizure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa13eb1",
   "metadata": {},
   "source": [
    "timeline diagram (preictal → ictal → postictal → interictal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb45a9",
   "metadata": {},
   "source": [
    "1. Ictal\n",
    "\n",
    "Meaning: “During a seizure.”\n",
    "\n",
    "In medical terms, “ictus” = seizure.\n",
    "\n",
    "So, ictal EEG = EEG recorded while the patient is actively experiencing a seizure.\n",
    "\n",
    "Characteristics:\n",
    "\n",
    "Sudden, abnormal, high-amplitude discharges.\n",
    "\n",
    "Spikes, sharp waves, rhythmic activity.\n",
    "\n",
    "Distinct from normal background EEG.\n",
    "\n",
    "2. Pre-ictal\n",
    "\n",
    "Meaning: The time just before a seizure begins.\n",
    "\n",
    "Sometimes considered the “warning phase.”\n",
    "\n",
    "Researchers are especially interested here → because predicting seizures requires recognizing pre-ictal patterns.\n",
    "\n",
    "Characteristics:\n",
    "\n",
    "Gradual build-up of abnormal discharges.\n",
    "\n",
    "Subtle shifts in frequency or synchrony.\n",
    "\n",
    "3. Inter-ictal\n",
    "\n",
    "Meaning: The period between seizures (patient is not seizing).\n",
    "\n",
    "But still: there can be abnormal EEG signatures even when the patient is not having a seizure.\n",
    "\n",
    "Characteristics:\n",
    "\n",
    "May show “spike-and-wave” discharges.\n",
    "\n",
    "Background EEG can be relatively normal or mildly disturbed.\n",
    "\n",
    "This is important because an inter-ictal patient can look clinically normal but EEG shows epileptiform activity.\n",
    "\n",
    "\n",
    "4. Normal (Baseline/Healthy)\n",
    "\n",
    "EEG of subjects with no epilepsy at all.\n",
    "\n",
    "Used as control group in datasets.\n",
    "\n",
    "\n",
    "5. Bonn Dataset Sets (Z, O, N, F, S)\n",
    "\n",
    "Specifically in the Bonn dataset, which you are working on:\n",
    "\n",
    "Set Z: Healthy, eyes open.\n",
    "\n",
    "Set O: Healthy, eyes closed.\n",
    "\n",
    "Set N: Interictal (epileptic patient, opposite hemisphere, not seizing).\n",
    "\n",
    "Set F: Interictal (epileptic patient, within epileptogenic zone, not seizing).\n",
    "\n",
    "Set S: Ictal (epileptic seizure activity).\n",
    "\n",
    "So:\n",
    "\n",
    "Z, O = healthy (normal EEGs)\n",
    "\n",
    "N, F = interictal (abnormal but no seizure)\n",
    "\n",
    "S = ictal (seizure present)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
